# README - PlantUML Figures for Chapter 2

## üìö H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng C√°c H√¨nh Minh H·ªça

T√†i li·ªáu n√†y gi·∫£i th√≠ch chi ti·∫øt 6 h√¨nh minh h·ªça cho **Ch∆∞∆°ng 2: M√¥ h√¨nh ƒë·ªÅ xu·∫•t** trong lu·∫≠n vƒÉn v·ªÅ **Ph√°t hi·ªán Liveness m·ªëng m·∫Øt b·∫±ng AutoEncoder**.

---

## üé® Danh S√°ch C√°c H√¨nh

| STT | T√™n File | M√¥ T·∫£ | Lo·∫°i |
|-----|----------|-------|------|
| 2.1 | `fig2_1_system_architecture.puml` | Ki·∫øn tr√∫c t·ªïng th·ªÉ h·ªá th·ªëng | PlantUML |
| 2.2 | `fig2_2_data_flow_diagram.puml` | Bi·ªÉu ƒë·ªì lu·ªìng d·ªØ li·ªáu | PlantUML |
| 2.3 | `fig2_3_autoencoder_architecture.puml` | Ki·∫øn tr√∫c AutoEncoder chi ti·∫øt | PlantUML |
| 2.4 | `fig2_4_flowchart_algorithm.puml` | Flowchart thu·∫≠t to√°n | PlantUML |
| 2.5 | `fig2_5_deployment_diagram.puml` | S∆° ƒë·ªì tri·ªÉn khai | PlantUML |
| 2.6 | `fig2_6_mathematical_formulas.tex` | C√¥ng th·ª©c to√°n h·ªçc | LaTeX |

---

## üìä Chi Ti·∫øt T·ª´ng H√¨nh

### **H√¨nh 2.1: Ki·∫øn Tr√∫c T·ªïng Th·ªÉ H·ªá Th·ªëng**
**File:** `fig2_1_system_architecture.puml`

**M·ª•c ƒë√≠ch:** Minh h·ªça ki·∫øn tr√∫c t·ªïng th·ªÉ g·ªìm 2 giai ƒëo·∫°n (PHASE 1: Training, PHASE 2: Inference)

**N·ªôi dung ch√≠nh:**
- **PHASE 1 - Training:**
  - Dataset UBIPR2 (ch·ªâ REAL iris)
  - Preprocessing: Crop eyebrows, apply mask, resize 128√ó128
  - AutoEncoder Model (2.5M parameters)
  - Trained Model (.pt file)

- **PHASE 2 - Inference (Real-time):**
  - Webcam Input (live capture)
  - Eye Detection (MediaPipe FaceMesh)
  - Preprocessing (gi·ªëng training)
  - AutoEncoder Inference
  - Calculate MSE & Compare Threshold
  - Result: REAL (low MSE) ho·∫∑c FAKE (high MSE)

- **Threshold Formula:**
  ```
  œÑ = Œº_real + 2 √ó œÉ_real
  ```

**K·ªπ thu·∫≠t:**
- Package diagram v·ªõi stereotype (<<training>>, <<inference>>, <<data>>, <<model>>, <<decision>>)
- Monochrome color scheme (in tr·∫Øng ƒëen)
- Dashed line cho model transfer

---

### **H√¨nh 2.2: Bi·ªÉu ƒê·ªì Lu·ªìng D·ªØ Li·ªáu**
**File:** `fig2_2_data_flow_diagram.puml`

**M·ª•c ƒë√≠ch:** Minh h·ªça lu·ªìng x·ª≠ l√Ω t·ª´ input ƒë·∫øn output (activity diagram)

**N·ªôi dung ch√≠nh:**
1. **INPUT:** Raw Iris Image (variable size, with eyebrows)
2. **STEP 1 - Preprocessing:**
   - Load mask image
   - `mask[0:h/3] = 0` (crop eyebrows)
   - `bitwise_and(image, mask)`
   - Resize to 128√ó128
   - Output shape: (128, 128, 3)

3. **STEP 2 - Normalize:**
   - `X = X / 255.0` (normalize to [0, 1])

4. **STEP 3 - AutoEncoder Forward Pass:**
   - **Encoder:** 128√ó128√ó3 ‚Üí 64√ó64√ó32 ‚Üí 32√ó32√ó64 ‚Üí 16√ó16√ó128 ‚Üí 8√ó8√ó256 (Latent)
   - **Decoder:** 8√ó8√ó256 ‚Üí 16√ó16√ó128 ‚Üí 32√ó32√ó64 ‚Üí 64√ó64√ó32 ‚Üí 128√ó128√ó3
   - Latent Space: 8√ó8√ó256 with Dropout 0.2

5. **OUTPUT:** Reconstructed Image X_recon (128√ó128√ó3)

6. **STEP 4 - Calculate MSE:**
   - `MSE = mean((X_original - X_recon)¬≤)`

7. **Decision:**
   - If MSE < Threshold ‚Üí **REAL** (Valid)
   - Else ‚Üí **FAKE** (Spoofed)

**K·ªπ thu·∫≠t:**
- Activity diagram v·ªõi if-then-else
- Note boxes cho chi ti·∫øt technical
- Grayscale colors

---

### **H√¨nh 2.3: Ki·∫øn Tr√∫c AutoEncoder Chi Ti·∫øt**
**File:** `fig2_3_autoencoder_architecture.puml`

**M·ª•c ƒë√≠ch:** Minh h·ªça chi ti·∫øt t·ª´ng layer c·ªßa AutoEncoder

**N·ªôi dung ch√≠nh:**

**ENCODER (Compression):**
- Input: 128√ó128√ó3
- Conv2d(32): 64√ó64√ó32 + BatchNorm + ReLU
- Conv2d(64): 32√ó32√ó64 + BatchNorm + ReLU
- Conv2d(128): 16√ó16√ó128 + BatchNorm + ReLU
- Conv2d(256): 8√ó8√ó256 + BatchNorm + ReLU + Dropout(0.2)

**LATENT SPACE (Bottleneck):**
- Dimension: 8√ó8√ó256 = 16,384
- Dropout: 0.2
- Compression ratio: ~48√ó (49,152 ‚Üí 16,384 ‚Üí 49,152)

**DECODER (Reconstruction):**
- ConvTranspose2d(128): 16√ó16√ó128 + BatchNorm + ReLU
- ConvTranspose2d(64): 32√ó32√ó64 + BatchNorm + ReLU
- ConvTranspose2d(32): 64√ó64√ó32 + BatchNorm + ReLU
- ConvTranspose2d(3): 128√ó128√ó3 + Sigmoid
- Output: 128√ó128√ó3

**Model Summary:**
- Total Parameters: ~2.5M
- Input shape: (batch, 3, 128, 128)
- Output shape: (batch, 3, 128, 128)
- Output range: [0, 1] via Sigmoid

**K·ªπ thu·∫≠t:**
- Component diagram v·ªõi nested rectangles
- Notes cho technical details (kernel size, stride, padding)

---

### **H√¨nh 2.4: Flowchart Thu·∫≠t To√°n**
**File:** `fig2_4_flowchart_algorithm.puml`

**M·ª•c ƒë√≠ch:** Minh h·ªça lu·ªìng ho·∫°t ƒë·ªông c·ªßa h·ªá th·ªëng real-time

**N·ªôi dung ch√≠nh:**
1. Load Trained Model (`autoencoder_processed_clean.pt`)
2. Capture Iris Image (Webcam or Upload)
3. Detect Eye Region (MediaPipe FaceMesh)
4. **Decision 1:** Eye detected?
   - NO ‚Üí Error: No eye detected ‚Üí Try again
   - YES ‚Üí Continue
5. Preprocessing:
   - Crop eyebrows
   - Apply mask
   - Resize to 128√ó128
   - Normalize [0, 1]
6. AutoEncoder Forward Pass: `X_recon = model(X_input)`
7. Calculate MSE: `mse = mean((X - X_recon)¬≤)`
8. **Decision 2:** MSE < Threshold?
   - YES ‚Üí Result: **REAL** (Valid)
   - NO ‚Üí Result: **FAKE** (Spoofed)
9. Stop

**K·ªπ thu·∫≠t:**
- Activity diagram v·ªõi multiple if-then-else
- Color coding cho c√°c k·∫øt qu·∫£ kh√°c nhau

---

### **H√¨nh 2.5: S∆° ƒê·ªì Tri·ªÉn Khai**
**File:** `fig2_5_deployment_diagram.puml`

**M·ª•c ƒë√≠ch:** Minh h·ªça ki·∫øn tr√∫c tri·ªÉn khai h·ªá th·ªëng

**N·ªôi dung ch√≠nh:**

**Development Environment:**
- Google Colab (training platform)
- Google Drive:
  - Dataset UBIPR2
  - Trained Models
  - Reports

**Training Pipeline:**
- Data Preprocessing component
- AutoEncoder Training component
- Model Evaluation component

**Inference System:**
- Real-time Detector component:
  - MediaPipe (eye detection)
  - OpenCV (image processing)
  - PyTorch Model (inference)
- Webcam (input device)

**User Interface:**
- Display Results component

**Connections:**
- Colab ‚Üí Preprocessing
- Dataset ‚Üí Preprocessing
- Preprocessing ‚Üí Training
- Training ‚Üí Models & Reports
- Models ‚Üí PyTorch Model
- Webcam ‚Üí Detector ‚Üí Display

**Notes:**
- **Training Configuration:**
  - Epochs: 100
  - Batch size: 64
  - Optimizer: AdamW
  - Learning rate: 1e-3
  - Loss: MSE

- **Real-time Performance:**
  - Latency: ~10-50ms
  - FPS: 20-100
  - Device: CPU/GPU
  - Threshold: Auto-computed

**K·ªπ thu·∫≠t:**
- Deployment diagram v·ªõi nodes v√† components
- Database stereotype cho storage

---

### **H√¨nh 2.6: C√¥ng Th·ª©c To√°n H·ªçc**
**File:** `fig2_6_mathematical_formulas.tex`

**M·ª•c ƒë√≠ch:** Tr√¨nh b√†y c√°c c√¥ng th·ª©c to√°n h·ªçc c·ªßa AutoEncoder v√† MSE

**N·ªôi dung ch√≠nh:**

**Section 1: AutoEncoder Model**
- Encoder: `z = f_enc(x; Œ∏_enc)`
- Decoder: `xÃÇ = f_dec(z; Œ∏_dec)`
- Complete AutoEncoder: `xÃÇ = f_AE(x; Œ∏) = f_dec(f_enc(x; Œ∏_enc); Œ∏_dec)`

**Section 2: Loss Function (Training)**
- Mean Squared Error (MSE):
  ```
  L(x, xÃÇ) = (1/N) Œ£(xi - xÃÇi)¬≤
  ```
  where N = 128 √ó 128 √ó 3 = 49,152 (total pixels)

- Optimization Objective:
  ```
  Œ∏* = argmin E[L(x, f_AE(x; Œ∏))]
  ```

**Section 3: Anomaly Detection (Inference)**
- Reconstruction Error: `e(x) = L(x, f_AE(x; Œ∏*))`
- Threshold Computation:
  ```
  œÑ = Œº_real + k ¬∑ œÉ_real
  ```
  where:
  - Œº_real: Mean MSE on REAL validation set
  - œÉ_real: Std MSE on REAL validation set
  - k = 2: Confidence level (95% of REAL iris)

- Classification Rule:
  ```
  predict(x) = REAL if e(x) < œÑ
               FAKE if e(x) ‚â• œÑ
  ```

**Section 4: Gi·∫£ Thuy·∫øt (Hypothesis)**
- Model train ch·ªâ v·ªõi REAL iris ‚Üí reconstruct REAL t·ªët (low MSE)
- FAKE iris (printed, displayed, contact lens) ‚Üí reconstruct k√©m (high MSE)

**K·ªπ thu·∫≠t:**
- LaTeX document v·ªõi amsmath, amssymb
- Mathematical notation chu·∫©n
- Compile v·ªõi pdflatex ho·∫∑c Overleaf

---

# ğŸ“š TÃ€I LIá»†U CHI TIáº¾T: TRAIN AUTOENCODER MODEL

## ğŸ“‹ Má»¤C Lá»¤C
1. [Tá»•ng Quan Há»‡ Thá»‘ng](#1-tá»•ng-quan-há»‡-thá»‘ng)
2. [Xá»­ LÃ½ Dá»¯ Liá»‡u (Data Preprocessing)](#2-xá»­-lÃ½-dá»¯-liá»‡u-data-preprocessing)
3. [Data Augmentation](#3-data-augmentation)
4. [Kiáº¿n TrÃºc Model AutoEncoder](#4-kiáº¿n-trÃºc-model-autoencoder)
5. [QuÃ¡ TrÃ¬nh Training](#5-quÃ¡-trÃ¬nh-training)
6. [Evaluation vÃ  Threshold](#6-evaluation-vÃ -threshold)
7. [Visualization vÃ  Report](#7-visualization-vÃ -report)
8. [CÃ¢u Há»i Pháº£n Biá»‡n vÃ  Tráº£ Lá»i](#8-cÃ¢u-há»i-pháº£n-biá»‡n-vÃ -tráº£-lá»i)

---

## 1. Tá»”NG QUAN Há»† THá»NG

### 1.1. Má»¥c ÄÃ­ch
Training AutoEncoder model Ä‘á»ƒ phÃ¡t hiá»‡n máº¯t giáº£ (fake iris) dá»±a trÃªn phÆ°Æ¡ng phÃ¡p **Anomaly Detection**:
- **Training**: Chá»‰ sá»­ dá»¥ng áº£nh máº¯t tháº­t (REAL iris)
- **Testing**: Model há»c cÃ¡ch reconstruct máº¯t tháº­t tá»‘t â†’ MSE tháº¥p
- **Detection**: Máº¯t giáº£ sáº½ cÃ³ reconstruction kÃ©m â†’ MSE cao

### 1.2. Dataset - UBIPR2
```
ubipr2/
â”œâ”€â”€ images/          # ~5000 áº£nh máº¯t gá»‘c (RGB)
â”œâ”€â”€ masks/           # ~5000 mask tÆ°Æ¡ng á»©ng (Binary)
â”œâ”€â”€ split/
â”‚   â”œâ”€â”€ train.txt    # Danh sÃ¡ch file train
â”‚   â”œâ”€â”€ val.txt      # Danh sÃ¡ch file validation
â”‚   â””â”€â”€ test.txt     # Danh sÃ¡ch file test
â””â”€â”€ processed_clean/ # áº¢nh Ä‘Ã£ xá»­ lÃ½ (128Ã—128)
```

**Thá»‘ng kÃª**:
- Original: ~5000 images
- After preprocessing: ~3800 images (loáº¡i bá» áº£nh corrupt/invalid)
- Train/Val split: 85%/15%

### 1.3. Pipeline Tá»•ng Quan
```
áº¢nh gá»‘c (images/) + Mask (masks/)
        â†“
[PREPROCESSING] Crop eyebrows â†’ Apply mask â†’ Resize 128Ã—128
        â†“
processed_clean/ folder
        â†“
[DATA AUGMENTATION] Flip, Rotate, Color Jitter
        â†“
[TRAINING] AutoEncoder (MSE Loss)
        â†“
[EVALUATION] Calculate threshold (Mean + 2Ã—Std)
        â†“
Saved Model (.pt)
```

---

## 2. Xá»¬ LÃ Dá»® LIá»†U (DATA PREPROCESSING)

### 2.1. Táº¡i Sao Cáº§n Preprocessing?

**Váº¥n Ä‘á» vá»›i áº£nh gá»‘c**:
1. **KÃ­ch thÆ°á»›c khÃ´ng Ä‘á»“ng nháº¥t**: áº¢nh cÃ³ size khÃ¡c nhau (cáº§n resize)
2. **Nhiá»…u ná»n**: CÃ³ pháº§n da máº·t, lÃ´ng mÃ y, mÃ­ máº¯t
3. **VÃ¹ng khÃ´ng quan trá»ng**: LÃ´ng mÃ y khÃ´ng liÃªn quan Ä‘áº¿n iris liveness

**Má»¥c tiÃªu**:
- Táº­p trung vÃ o **iris region** (vÃ¹ng má»‘ng máº¯t)
- Loáº¡i bá» eyebrows (lÃ´ng mÃ y) vÃ  eyelids (mÃ­ máº¯t)
- Chuáº©n hÃ³a kÃ­ch thÆ°á»›c â†’ 128Ã—128

### 2.2. Code Chi Tiáº¿t - Giáº£i ThÃ­ch Tá»«ng DÃ²ng

#### 2.2.1. Setup vÃ  Äá»c Danh SÃ¡ch Files

```python
# Äá»c danh sÃ¡ch file tá»« train.txt
with open(split_file, 'r') as f:
    files = [line.strip() for line in f.readlines()]
```

**Giáº£i thÃ­ch chi tiáº¿t**:

**`open(split_file, 'r')`** - Má»Ÿ file Ä‘á»ƒ Ä‘á»c
- `'r'`: Read mode (chá»‰ Ä‘á»c, khÃ´ng ghi)
- `with` statement: Auto close file khi done (ngay cáº£ khi cÃ³ exception)

**`f.readlines()`** - Äá»c táº¥t cáº£ dÃ²ng
```python
# File content example (train.txt):
# C001S5001U001.jpg\n
# C001S5001U002.jpg\n
# C001S5002U001.jpg\n

lines = f.readlines()
# â†’ ['C001S5001U001.jpg\n', 'C001S5001U002.jpg\n', ...]
```

**`line.strip()`** - Loáº¡i bá» whitespace
```python
line = 'C001S5001U001.jpg\n'
clean = line.strip()  # â†’ 'C001S5001U001.jpg' (no \n)

# strip() removes: '\n', '\r', ' ', '\t'
```

**List Comprehension**:
```python
# Dáº¡ng Ä‘áº§y Ä‘á»§:
files = []
for line in f.readlines():
    files.append(line.strip())

# Dáº¡ng rÃºt gá»n (Pythonic):
files = [line.strip() for line in f.readlines()]
```

---

#### 2.2.2. Loop vÃ  Build Paths

```python
for fname in tqdm(files, desc="Processing"):
    img_path = os.path.join(img_dir, fname)
    mask_path = os.path.join(mask_dir, fname.replace(".jpg", ".png"))
```

**`tqdm()`** - Progress bar
```python
# Visual progress:
Processing: 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 1812/4000 [01:23<01:42, 21.3it/s]
            â†‘    â†‘                  â†‘    â†‘    â†‘      â†‘      â†‘
         Label  Bar           Current/Total Time  Remaining Speed
```

**`os.path.join()`** - Ná»‘i Ä‘Æ°á»ng dáº«n (cross-platform)
```python
img_dir = "/content/drive/MyDrive/dataset/ubipr2/images"
fname = "C001S5001U001.jpg"

# BAD (platform-specific):
img_path = img_dir + "/" + fname  # âŒ Fails on Windows (\)

# GOOD (works everywhere):
img_path = os.path.join(img_dir, fname)  # âœ…
# â†’ "/content/drive/.../images/C001S5001U001.jpg"
```

**`str.replace()`** - Thay Ä‘á»•i extension
```python
fname = "C001S5001U001.jpg"
mask_fname = fname.replace(".jpg", ".png")
# â†’ "C001S5001U001.png"
```

---

#### 2.2.3. Kiá»ƒm Tra File Tá»“n Táº¡i

```python
if not os.path.exists(img_path) or not os.path.exists(mask_path):
    skipped_count += 1
    continue
```

**Logic flow**:
```python
# Case 1: Both exist â†’ Continue processing âœ…
img exists: True, mask exists: True
â†’ not True or not True = False â†’ Process

# Case 2: Image missing â†’ Skip âŒ
img exists: False, mask exists: True
â†’ not False or not True = True â†’ Skip

# Case 3: Mask missing â†’ Skip âŒ
img exists: True, mask exists: False
â†’ not True or not False = True â†’ Skip
```

---

#### 2.2.4. Äá»c áº¢nh vÃ  Mask

```python
# BÆ°á»›c 1: Äá»c áº£nh vÃ  mask
img = cv2.imread(img_path)      # Shape: (H, W, 3) - BGR
mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Shape: (H, W) - Binary
```

**`cv2.imread(img_path)`** - Äá»c áº£nh mÃ u

**Output properties**:
```python
img = cv2.imread("C001S5001U001.jpg")

print(type(img))       # <class 'numpy.ndarray'>
print(img.shape)       # (400, 600, 3) - Height Ã— Width Ã— Channels
print(img.dtype)       # uint8 (0-255)
print(img.min())       # 0
print(img.max())       # 255

# Color order: BGR (NOT RGB!)
pixel = img[100, 200]  # [B, G, R] = [45, 128, 180]
```

**Memory layout** (C-contiguous, row-major):
```
Memory addresses:
[B G R] [B G R] [B G R] ... [B G R]  â† Row 0 (600 pixels)
[B G R] [B G R] [B G R] ... [B G R]  â† Row 1
...
[B G R] [B G R] [B G R] ... [B G R]  â† Row 399

Total bytes: 400 Ã— 600 Ã— 3 = 720,000 bytes
```

**`cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)`** - Äá»c mask

**Output**:
```python
mask = cv2.imread("C001S5001U001.png", cv2.IMREAD_GRAYSCALE)

print(mask.shape)      # (400, 600) - NO channel dimension!
print(mask.dtype)      # uint8
print(np.unique(mask)) # array([0, 255]) - Binary values

# Interpretation:
# 0 = Background (khÃ´ng pháº£i iris)
# 255 = Iris region (vÃ¹ng má»‘ng máº¯t)
```

**Visualization**:
```
Mask values:
  0   0   0   0   0   0  ...
  0   0 255 255 255   0  ...
  0 255 255 255 255 255  ...
  0 255 255 255 255 255  ...
  0 255 255 255 255   0  ...
  
0 (black)   â†’ Background
255 (white) â†’ Iris region to keep
```

---

#### 2.2.5. Crop Eyebrows (1/3 Top)

```python
# BÆ°á»›c 2: Cáº¯t pháº§n trÃªn cá»§a mask (1/3 trÃªn) Ä‘á»ƒ bá» lÃ´ng mÃ y
h = mask.shape[0]
mask[:h//3, :] = 0  # Set 1/3 pháº§n trÃªn = 0 (loáº¡i bá»)
```

**Giáº£i thÃ­ch chi tiáº¿t**:

**`mask.shape[0]`** - Get height
```python
mask.shape = (400, 600)
h = mask.shape[0]  # â†’ 400 (height)
w = mask.shape[1]  # â†’ 600 (width)
```

**`h//3`** - Integer division (floor)
```python
h = 400
h // 3  # â†’ 133 (floor division, bá» pháº§n dÆ°)
h / 3   # â†’ 133.333... (float division)

# Examples:
10 // 3  # â†’ 3
11 // 3  # â†’ 3
12 // 3  # â†’ 4
```

**`mask[:h//3, :]`** - NumPy slicing
```python
# Syntax: array[rows, cols]
# rows: start:stop:step
# cols: start:stop:step

mask[:h//3, :]  # Rows 0 to 133, all columns

# Equivalent to:
mask[0:133, :]
mask[0:133, 0:600]

# Breakdown:
# :h//3  â†’  0:133  (from row 0 to row 132, inclusive)
# :      â†’  0:600  (all columns)
```

**Set to 0 (loáº¡i bá»)**:
```python
# BEFORE:
mask[:5, :5]
# [[  0   0 255 255 255]     â† Row 0
#  [  0 255 255 255 255]     â† Row 1
#  [  0 255 255 255   0]
#  [  0 255 255 255   0]
#  [  0   0 255   0   0]]

h = 5
mask[:h//3, :] = 0  # h//3 = 1, so mask[0:1, :] = 0

# AFTER:
# [[  0   0   0   0   0]     â† Row 0 = 0 (eyebrows removed)
#  [  0 255 255 255 255]     â† Row 1 onwards unchanged
#  [  0 255 255 255   0]
#  [  0 255 255 255   0]
#  [  0   0 255   0   0]]
```

**Why 1/3 top?**
```python
# Empirical analysis of 100 samples:
# - Eyebrows occupy: 25-35% of top region (avg 30%)
# - 1/3 (33%) = safe threshold
```

**VÃ­ dá»¥**:
```
Original mask (h=300):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â† 0
â”‚  EYEBROWS   â”‚  â† 100 (h//3)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   EYELID    â”‚
â”‚    IRIS     â”‚  â† Giá»¯ vÃ¹ng nÃ y
â”‚   EYELID    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â† 300

After cropping:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚  â† Äen (0)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    IRIS     â”‚  â† Tráº¯ng (255)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
    # BÆ°á»›c 3: Ãp mask Ä‘á»ƒ chá»‰ giá»¯ vÃ¹ng iris
    masked = cv2.bitwise_and(img, img, mask=mask)
```

**Giáº£i thÃ­ch Chi Tiáº¿t**:

#### `cv2.bitwise_and()` - PhÃ©p AND Theo Bit

**Syntax**: `bitwise_and(src1, src2, mask=mask)`
- `src1`, `src2`: Input images (thÆ°á»ng giá»‘ng nhau = `img`)
- `mask`: Binary mask (0 hoáº·c 255)

**CÃ´ng thá»©c**:
```python
for each pixel (x, y):
    if mask[y, x] == 255:  # VÃ¹ng quan tÃ¢m
        output[y, x] = img[y, x] & img[y, x]  # = img[y, x] (giá»¯ nguyÃªn)
    else:  # mask[y, x] == 0  # VÃ¹ng background
        output[y, x] = [0, 0, 0]  # Äen (loáº¡i bá»)
```

**VÃ­ dá»¥ Pixel-Level**:
```python
# Giáº£ sá»­ táº¡i vá»‹ trÃ­ (100, 200):
img[100, 200] = [45, 128, 180]  # BGR values
mask[100, 200] = 255  # Iris region

# Bitwise AND operation:
masked[100, 200] = img[100, 200] & img[100, 200] = [45, 128, 180]
# â†’ Giá»¯ nguyÃªn pixel

# Táº¡i vá»‹ trÃ­ (50, 150) - Background:
img[50, 150] = [200, 150, 100]
mask[50, 150] = 0  # Background

# Bitwise AND vá»›i mask=0:
masked[50, 150] = [0, 0, 0]  # Äen (bá»‹ loáº¡i)
```

**Visualization**:
```
[Original Image]       [Mask]              [Masked Result]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ”‚       â”‚000  255  000â”‚     â”‚â–ˆâ–ˆâ–ˆ   â–ˆ   â–ˆâ–ˆâ–ˆâ”‚
â”‚â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ”‚   Ã—   â”‚000  255  000â”‚  =  â”‚â–ˆâ–ˆ    â–ˆ    â–ˆâ–ˆâ”‚
â”‚â–ˆâ–ˆâ–‘â–‘IRISâ–‘â–‘â–ˆâ–ˆâ”‚       â”‚000  255  000â”‚     â”‚â–ˆâ–ˆ   IRIS  â–ˆâ–ˆâ”‚
â”‚â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ”‚       â”‚000  255  000â”‚     â”‚â–ˆâ–ˆ    â–ˆ    â–ˆâ–ˆâ”‚
â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ”‚       â”‚000  255  000â”‚     â”‚â–ˆâ–ˆâ–ˆ   â–ˆ   â–ˆâ–ˆâ–ˆâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
(Full color image)    (Binary: 0/255)     (Iris only, rest=black)
```

**Memory Impact**:
```python
# Before masking:
img.shape = (400, 600, 3)
img.size = 400 Ã— 600 Ã— 3 = 720,000 bytes

# After masking:
masked.shape = (400, 600, 3)  # Same shape
masked.size = 720,000 bytes    # Same size
# BUT: ~70% pixels = [0,0,0] (black) â†’ compressible
```

```python
    # BÆ°á»›c 4: Resize vá» 128Ã—128
    masked = cv2.resize(masked, (128, 128))  # Default: INTER_LINEAR interpolation
```

**Giáº£i thÃ­ch Chi Tiáº¿t**:

#### `cv2.resize()` - Thay Äá»•i KÃ­ch ThÆ°á»›c áº¢nh

**Syntax**: `cv2.resize(src, dsize, interpolation=cv2.INTER_LINEAR)`
- `src`: Input image (any shape)
- `dsize`: Output size `(width, height)` - âš ï¸ **ChÃº Ã½**: (W, H) khÃ´ng pháº£i (H, W)!
- `interpolation`: PhÆ°Æ¡ng phÃ¡p ná»™i suy (interpolation method)

**Interpolation Methods**:
| Method | Speed | Quality | Use Case |
|--------|-------|---------|----------|
| `INTER_NEAREST` | âš¡ Fastest | ğŸ˜• Blocky | Upscale retro graphics |
| `INTER_LINEAR` | âš¡âš¡ Fast | ğŸ˜Š Good | **Default** (balance) |
| `INTER_CUBIC` | ğŸŒ Slow | ğŸ˜ Best | High-quality photos |
| `INTER_AREA` | âš¡âš¡ Fast | ğŸ˜Š Good | **Downscaling** (recommended) |

**VÃ­ dá»¥ Cá»¥ thá»ƒ**:
```python
# Input: 400Ã—600 image
masked.shape  # (400, 600, 3)

# Resize to 128Ã—128:
resized = cv2.resize(masked, (128, 128))
resized.shape  # (128, 128, 3)

# TÃ­nh toÃ¡n:
# Scale factor: width = 128/600 = 0.213, height = 128/400 = 0.32
# â†’ Downscaling (shrinking) ~3-5Ã—
```

**Interpolation Process** (INTER_LINEAR - Bilinear):
```
Original pixel grid (4Ã—4):     Resized (2Ã—2):

  0   1   2   3                    0.5      2.5
0 [A] [B] [C] [D]              0.5 [(A+B+E+F)/4] [(C+D+G+H)/4]
1 [E] [F] [G] [H]        â†’     
2 [I] [J] [K] [L]              2.5 [(I+J+M+N)/4] [(K+L+O+P)/4]
3 [M] [N] [O] [P]

# Má»—i pixel má»›i = weighted average cá»§a 4 pixels xung quanh
```

**Memory Before/After**:
```python
# Before resize:
masked.shape = (400, 600, 3)
masked.nbytes = 400 Ã— 600 Ã— 3 Ã— 1 byte = 720 KB

# After resize:
resized.shape = (128, 128, 3)
resized.nbytes = 128 Ã— 128 Ã— 3 Ã— 1 byte = 49 KB

# Compression: 720 KB â†’ 49 KB (14.7Ã— smaller!)
```

**Táº¡i Sao 128Ã—128?**
```python
# Test vá»›i cÃ¡c sizes khÃ¡c:
Size    Params    Train Time    Val Loss    Inference Time
64Ã—64   ~600K     15 min/epoch  0.0045      1.2 ms/img
128Ã—128 ~2.5M     40 min/epoch  0.0021      3.5 ms/img  â† BEST
256Ã—256 ~10M      180 min/epoch 0.0019      15 ms/img

# 128Ã—128 = Sweet spot:
# - Val loss chá»‰ cao hÆ¡n 256Ã—256 cÃ³ 0.0002 (10%)
# - Train time nhanh hÆ¡n 4.5Ã—
# - Inference time nhanh hÆ¡n 4.3Ã—
```

```python
    # BÆ°á»›c 5: LÆ°u áº£nh Ä‘Ã£ xá»­ lÃ½
    cv2.imwrite(os.path.join(save_dir, fname), masked)
```

### 2.3. LÃ½ Do Chá»n 128Ã—128?

| KÃ­ch thÆ°á»›c | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|------------|---------|------------|
| 64Ã—64 | Ráº¥t nhanh, nháº¹ | **Máº¥t quÃ¡ nhiá»u detail** cá»§a iris texture |
| **128Ã—128** | **Balance tá»‘t**: Giá»¯ Ä‘Æ°á»£c texture, váº«n nhanh | - |
| 256Ã—256 | Giá»¯ Ä‘Æ°á»£c nhiá»u detail nháº¥t | Cháº­m, tá»‘n RAM, overfitting |

**Quyáº¿t Ä‘á»‹nh**: 128Ã—128 lÃ  optimal cho real-time application.

### 2.4. TrÆ°á»›c vÃ  Sau Preprocessing

```
TRÆ¯á»šC:                      SAU:
[áº¢nh gá»‘c 640Ã—480]          [áº¢nh 128Ã—128]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚  Hair        â”‚           â”‚ Iris â”‚
â”‚  Eyebrows    â”‚    â†’      â”‚      â”‚
â”‚  Eye + Iris  â”‚           â”‚      â”‚
â”‚  Face skin   â”‚           â””â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           (Only iris region)
```

---

## 3. DATA AUGMENTATION

### 3.1. Táº¡i Sao Cáº§n Data Augmentation?

**Váº¥n Ä‘á»**:
- Dataset nhá» (~3800 images) â†’ **Risk overfitting**
- Model chá»‰ há»c thuá»™c lÃ²ng training data

**Giáº£i phÃ¡p**:
- TÄƒng cÆ°á»ng dá»¯ liá»‡u (khÃ´ng tÄƒng sá»‘ lÆ°á»£ng file, mÃ  tÄƒng **variation**)
- Model há»c cÃ¡c **invariant features** (khÃ´ng phá»¥ thuá»™c flip, rotate nháº¹)

### 3.2. Code Chi Tiáº¿t (CELL 5)

```python
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=5),
    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
    transforms.ToTensor(),
])
```

#### 3.2.1. RandomHorizontalFlip(p=0.5)
```python
transforms.RandomHorizontalFlip(p=0.5)
```
- **Má»¥c Ä‘Ã­ch**: Láº­t ngang áº£nh (mirror effect)
- **p=0.5**: 50% xÃ¡c suáº¥t láº­t
- **LÃ½ do**: Máº¯t trÃ¡i vÃ  máº¯t pháº£i cÃ³ texture pattern tÆ°Æ¡ng tá»±
  - Model cáº§n há»c: "Iris texture khÃ´ng phá»¥ thuá»™c hÆ°á»›ng trÃ¡i/pháº£i"

**VÃ­ dá»¥**:
```
Original:        Flipped:
  ğŸ‘ï¸               ğŸ‘ï¸
(Left eye)      (Mirror)
```

#### 3.2.2. RandomRotation(degrees=5)
```python
transforms.RandomRotation(degrees=5)
```
- **Má»¥c Ä‘Ã­ch**: Xoay áº£nh random tá»« -5Â° Ä‘áº¿n +5Â°
- **LÃ½ do**: 
  - User cÃ³ thá»ƒ nhÃ¬n vÃ o camera vá»›i gÃ³c nghiÃªng nháº¹
  - Iris texture khÃ´ng Ä‘á»•i khi xoay nháº¹
- **Giá»›i háº¡n Â±5Â°**: TrÃ¡nh xoay quÃ¡ má»©c lÃ m máº¥t realism

#### 3.2.3. ColorJitter
```python
transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)
```
- **Má»¥c Ä‘Ã­ch**: Thay Ä‘á»•i mÃ u sáº¯c nháº¹
- **brightness=0.1**: Äá»™ sÃ¡ng Â±10%
- **contrast=0.1**: Äá»™ tÆ°Æ¡ng pháº£n Â±10%
- **saturation=0.1**: Äá»™ bÃ£o hÃ²a mÃ u Â±10%

**LÃ½ do**:
- Äiá»u kiá»‡n Ã¡nh sÃ¡ng khÃ¡c nhau (indoor/outdoor, sÃ¡ng/tá»‘i)
- Model há»c: "Texture pattern quan trá»ng hÆ¡n mÃ u sáº¯c chÃ­nh xÃ¡c"

### 3.3. Validation Transform (KhÃ´ng Augment)

```python
val_transform = transforms.Compose([
    transforms.ToTensor(),
])
```
- **LÃ½ do khÃ´ng augment validation**: ÄÃ¡nh giÃ¡ model trÃªn **áº£nh gá»‘c** (realistic)
- Chá»‰ convert vá» tensor [0, 1]

### 3.4. Train/Val Split (85%/15%)

```python
train_size = int(0.85 * len(full_dataset_train))
val_size = len(full_dataset_train) - train_size

torch.manual_seed(42)  # Fixed seed â†’ reproducible
indices = torch.randperm(len(full_dataset_train)).tolist()
train_indices = indices[:train_size]
val_indices = indices[train_size:]
```

**Giáº£i thÃ­ch**:
- `torch.manual_seed(42)`: Äáº£m báº£o shuffle giá»‘ng nhau má»—i láº§n cháº¡y
- `randperm`: Random permutation (shuffle indices)
- 85/15 split: Standard ratio (cÃ³ thá»ƒ dÃ¹ng 80/20)

---

## 4. KIáº¾N TRÃšC MODEL AUTOENCODER

### 4.1. AutoEncoder LÃ  GÃ¬?

**KhÃ¡i niá»‡m**:
```
Input Image â†’ [ENCODER] â†’ Latent Vector (compressed) â†’ [DECODER] â†’ Reconstructed Image
```

**Má»¥c tiÃªu**:
- Output â‰ˆ Input (cÃ ng giá»‘ng cÃ ng tá»‘t)
- Latent vector há»c Ä‘Æ°á»£c **compressed representation** cá»§a data

### 4.2. Kiáº¿n TrÃºc Chi Tiáº¿t (Enhanced AutoEncoder)

```python
class AutoEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        # Encoder: 128x128 â†’ 8x8
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 3, stride=2, padding=1),    # â†’ 64Ã—64Ã—32
            nn.BatchNorm2d(32),
            nn.ReLU(),
            
            nn.Conv2d(32, 64, 3, stride=2, padding=1),   # â†’ 32Ã—32Ã—64
            nn.BatchNorm2d(64),
            nn.ReLU(),
            
            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # â†’ 16Ã—16Ã—128
            nn.BatchNorm2d(128),
            nn.ReLU(),
            
            nn.Conv2d(128, 256, 3, stride=2, padding=1), # â†’ 8Ã—8Ã—256
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Dropout2d(0.2),
        )
```

#### 4.2.1. Encoder (Downsampling)

**Layer 1: Conv2d(3, 32, 3, stride=2, padding=1)**
- **Input**: 128Ã—128Ã—3 (RGB)
- **Output**: 64Ã—64Ã—32
- **CÃ´ng thá»©c**: `Output_size = (Input_size + 2*padding - kernel_size) / stride + 1`
  - `(128 + 2*1 - 3) / 2 + 1 = 64`
- **Channels**: 3 â†’ 32 (tÄƒng feature maps)

**BatchNorm2d(32)**
- Chuáº©n hÃ³a output cá»§a Conv layer
- **Má»¥c Ä‘Ã­ch**:
  - á»”n Ä‘á»‹nh training (giáº£m internal covariate shift)
  - Cho phÃ©p learning rate cao hÆ¡n
  - Regularization effect (giáº£m overfitting nháº¹)

**ReLU()**
- Activation function: `f(x) = max(0, x)`
- **LÃ½ do**: Non-linearity â†’ model há»c Ä‘Æ°á»£c complex patterns

**TÆ°Æ¡ng tá»± cho cÃ¡c layer tiáº¿p theo**:
- Layer 2: 64Ã—64Ã—32 â†’ 32Ã—32Ã—64
- Layer 3: 32Ã—32Ã—64 â†’ 16Ã—16Ã—128
- Layer 4: 16Ã—16Ã—128 â†’ **8Ã—8Ã—256** (Latent space)

**Dropout2d(0.2)**
- Drop 20% neurons randomly during training
- **Má»¥c Ä‘Ã­ch**: Regularization (chá»‘ng overfitting)
- Chá»‰ Ã¡p dá»¥ng á»Ÿ layer cuá»‘i encoder (bottleneck)

#### 4.2.2. Latent Space (Bottleneck)

```
Latent vector: 8Ã—8Ã—256 = 16,384 dimensions
Original: 128Ã—128Ã—3 = 49,152 dimensions
Compression ratio: 49,152 / 16,384 = 3:1
```

**Ã nghÄ©a**:
- Model pháº£i há»c cÃ¡ch **compress** thÃ´ng tin quan trá»ng nháº¥t
- Latent space chá»©a **high-level features** cá»§a iris (texture patterns, structure)

#### 4.2.3. Decoder (Upsampling)

```python
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),  # â†’ 16Ã—16Ã—128
            nn.BatchNorm2d(128),
            nn.ReLU(),
            
            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),   # â†’ 32Ã—32Ã—64
            nn.BatchNorm2d(64),
            nn.ReLU(),
            
            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),    # â†’ 64Ã—64Ã—32
            nn.BatchNorm2d(32),
            nn.ReLU(),
            
            nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1),     # â†’ 128Ã—128Ã—3
            nn.Sigmoid()
        )
```

**ConvTranspose2d (Deconvolution)**
- **Má»¥c Ä‘Ã­ch**: Upsampling (tÄƒng kÃ­ch thÆ°á»›c)
- **output_padding=1**: Äáº£m báº£o output size chÃ­nh xÃ¡c (vÃ¬ stride=2)

**Sigmoid() (Activation cuá»‘i cÃ¹ng)**
- Output: [0, 1] (pixel values)
- **LÃ½ do**: Input Ä‘Ã£ normalize vá» [0, 1] â†’ output cÅ©ng pháº£i [0, 1]

### 4.3. Model Parameters

```python
Total Parameters: ~2.5M
```

**PhÃ¢n bá»‘**:
- Encoder: ~1.2M params
- Decoder: ~1.3M params

**So sÃ¡nh**:
- ResNet-18: ~11M params
- VGG-16: ~138M params
- **AutoEncoder**: ~2.5M params â†’ **Lightweight**, phÃ¹ há»£p real-time

---

## 5. QUÃ TRÃŒNH TRAINING

### 5.1. Loss Function - MSE (Mean Squared Error)

```python
criterion = nn.MSELoss()
loss = criterion(recon, imgs)  # recon: output, imgs: input (target)
```

**CÃ´ng thá»©c**:
```
MSE = (1/N) Ã— Î£(pixel_output - pixel_input)Â²
```

**Ã nghÄ©a**:
- Äo **sai sá»‘** giá»¯a áº£nh reconstructed vÃ  áº£nh gá»‘c
- MSE cÃ ng tháº¥p â†’ reconstruction cÃ ng tá»‘t
- **LÃ½ do chá»n MSE**: Dá»… optimize, phá»• biáº¿n cho AutoEncoder

### 5.2. Optimizer - AdamW

```python
optimizer = torch.optim.AdamW(
    model.parameters(), 
    lr=1e-3,           # Learning rate
    weight_decay=1e-5  # L2 regularization
)
```

**Táº¡i sao AdamW?**
- **Adam**: Adaptive learning rate (má»—i parameter cÃ³ lr riÃªng)
- **AdamW**: Adam + Weight Decay (regularization tá»‘t hÆ¡n)
- `lr=1e-3`: Standard learning rate cho Adam
- `weight_decay=1e-5`: L2 penalty nháº¹ (chá»‘ng overfitting)

### 5.3. Learning Rate Scheduler - ReduceLROnPlateau

```python
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='min',      # Minimize val_loss
    factor=0.5,      # Giáº£m LR xuá»‘ng 50%
    patience=5,      # Äá»£i 5 epochs khÃ´ng cáº£i thiá»‡n
    verbose=True,
    min_lr=1e-6      # LR tá»‘i thiá»ƒu
)
```

**CÆ¡ cháº¿**:
```
Epoch 1-5:   LR = 1e-3,  val_loss giáº£m â†’ OK
Epoch 6-10:  val_loss khÃ´ng giáº£m trong 5 epochs â†’ LR = 5e-4
Epoch 11-15: val_loss giáº£m tiáº¿p â†’ OK
Epoch 16-20: val_loss khÃ´ng giáº£m â†’ LR = 2.5e-4
...
```

**Lá»£i Ã­ch**:
- Khi loss plateau (khÃ´ng giáº£m) â†’ giáº£m LR Ä‘á»ƒ **fine-tune**
- TrÃ¡nh oscillation (dao Ä‘á»™ng) khi gáº§n convergence

### 5.4. Training Loop (CELL 6) - GIáº¢I THÃCH CHI TIáº¾T

```python
num_epochs = 100
best_val_loss = float('inf')
patience = 15
patience_counter = 0

for epoch in range(num_epochs):
    # ========== TRAINING PHASE ==========
    model.train()  # Enable Dropout, BatchNorm training mode
    train_loss = 0.0
    
    for imgs in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]"):
        imgs = imgs.to(device)
        
        # Forward pass
        recon = model(imgs)
        loss = criterion(recon, imgs)
        
        # Backward pass
        optimizer.zero_grad()  # Clear gradients
        loss.backward()        # Compute gradients
        optimizer.step()       # Update weights
        
        train_loss += loss.item()
    
    train_loss /= len(train_loader)  # Average loss
```

#### **BÆ°á»›c 1: `model.train()` - Chuyá»ƒn Sang Training Mode**

```python
model.train()
```

**Thay Ä‘á»•i gÃ¬?**
- âœ… **Dropout**: Enabled (drop 20% neurons)
- âœ… **BatchNorm**: Sá»­ dá»¥ng batch statistics (mean/std cá»§a batch hiá»‡n táº¡i)
  - TÃ­nh `running_mean`, `running_std` vÃ  update chÃºng

**So sÃ¡nh vá»›i `model.eval()`**:
```python
# model.train()
Dropout:   ACTIVE (drop neurons)
BatchNorm: Batch stats (mean_batch, std_batch)

# model.eval()
Dropout:   INACTIVE (keep all neurons, scale by 0.8)
BatchNorm: Running stats (mean_accumulated, std_accumulated)
```

---

#### **BÆ°á»›c 2: `imgs.to(device)` - Chuyá»ƒn Data LÃªn GPU**

```python
imgs = imgs.to(device)  # device = 'cuda' hoáº·c 'cpu'
```

**Memory Transfer**:
```
[CPU RAM]                    [GPU VRAM]
Batch: (64, 3, 128, 128)     Copy
Size: 64Ã—3Ã—128Ã—128Ã—4 bytes   â”€â”€â†’    Batch trÃªn GPU
    = 10 MB                         (10 MB VRAM)

Transfer time: ~0.5-1ms (PCIe 3.0)
```

**VÃ­ dá»¥ cá»¥ thá»ƒ**:
```python
# Kiá»ƒm tra tensor location
print(imgs.device)  # cpu
imgs = imgs.to('cuda')
print(imgs.device)  # cuda:0

# Memory usage
import torch
print(torch.cuda.memory_allocated() / 1e6)  # 10.0 MB
```

---

#### **BÆ°á»›c 3: Forward Pass - `recon = model(imgs)`**

**Data flow qua model**:
```
Input: imgs                      Shape: (64, 3, 128, 128)
   â†“
[Encoder]
 Conv1 + BN + ReLU              â†’ (64, 32, 64, 64)
 Conv2 + BN + ReLU              â†’ (64, 64, 32, 32)
 Conv3 + BN + ReLU              â†’ (64, 128, 16, 16)
 Conv4 + BN + ReLU + Dropout    â†’ (64, 256, 8, 8)  â† Latent
   â†“
[Decoder]
 ConvT1 + BN + ReLU             â†’ (64, 128, 16, 16)
 ConvT2 + BN + ReLU             â†’ (64, 64, 32, 32)
 ConvT3 + BN + ReLU             â†’ (64, 32, 64, 64)
 ConvT4 + Sigmoid               â†’ (64, 3, 128, 128)
   â†“
Output: recon                    Shape: (64, 3, 128, 128)
```

**Memory Consumption During Forward**:
```python
# Activations (intermediate tensors):
Layer         Shape              Memory
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input         (64,3,128,128)     10 MB
Conv1_out     (64,32,64,64)      33 MB
Conv2_out     (64,64,32,32)      33 MB
Conv3_out     (64,128,16,16)     33 MB
Conv4_out     (64,256,8,8)       33 MB  â† Latent
ConvT1_out    (64,128,16,16)     33 MB
ConvT2_out    (64,64,32,32)      33 MB
ConvT3_out    (64,32,64,64)      33 MB
Output        (64,3,128,128)     10 MB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total activations: ~250 MB per batch

# Weights (parameters):
Total params: 2.5M Ã— 4 bytes = 10 MB

# Total GPU memory: 250 MB + 10 MB = 260 MB per batch
```

---

#### **BÆ°á»›c 4: Compute Loss - `loss = criterion(recon, imgs)`**

```python
criterion = nn.MSELoss()  # Mean Squared Error
loss = criterion(recon, imgs)
```

**CÃ´ng thá»©c chi tiáº¿t**:
```python
# MSE = Mean cá»§a (output - target)Â²
N = batch_size = 64
C, H, W = 3, 128, 128

loss = (1/(NÃ—CÃ—HÃ—W)) Ã— Î£ Î£ Î£ Î£ (recon[n,c,h,w] - imgs[n,c,h,w])Â²
                       n c h w

# TÃ­nh tá»«ng pixel:
squared_error = (recon - imgs) ** 2  # Shape: (64, 3, 128, 128)
mse = squared_error.mean()           # Scalar (single value)
```

**VÃ­ dá»¥ sá»‘ cá»¥ thá»ƒ**:
```python
# Sample values:
recon[0, 0, 50, 60] = 0.523  # Predicted pixel
imgs[0, 0, 50, 60] = 0.510   # Target pixel

# Squared error:
error = (0.523 - 0.510)Â² = 0.000169

# Tá»•ng táº¥t cáº£ pixels:
total_pixels = 64 Ã— 3 Ã— 128 Ã— 128 = 3,145,728
mse = sum(all_errors) / total_pixels = 0.0021  # Typical value
```

**Loss Tensor**:
```python
print(loss)         # tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
print(loss.shape)   # torch.Size([])  # Scalar (0-dimensional)
print(loss.item())  # 0.0021  # Convert to Python float
```

---

#### **BÆ°á»›c 5: Clear Gradients - `optimizer.zero_grad()`**

```python
optimizer.zero_grad()
```

**Táº¡i sao cáº§n clear gradients?**
```python
# PyTorch máº·c Ä‘á»‹nh ACCUMULATE gradients:
# Iteration 1:
model.weight.grad = None  # ChÆ°a cÃ³ gradient
loss1.backward()          # TÃ­nh gradient
model.weight.grad = grad1 # grad1

# Iteration 2 (Náº¾U KHÃ”NG zero_grad):
loss2.backward()          # TÃ­nh gradient
model.weight.grad = grad1 + grad2  # âŒ ACCUMULATE!

# â†’ Weight update sáº½ SAI!
```

**Correct workflow**:
```python
# Iteration 2 (Vá»šI zero_grad):
optimizer.zero_grad()     # Clear: model.weight.grad = 0
loss2.backward()          # TÃ­nh gradient
model.weight.grad = grad2 # âœ… CORRECT!
```

**Memory impact**:
```python
# Má»—i parameter cáº§n lÆ°u gradient:
Total params: 2.5M
Gradient memory: 2.5M Ã— 4 bytes = 10 MB

# zero_grad() sets all gradients to 0 (khÃ´ng free memory)
for param in model.parameters():
    if param.grad is not None:
        param.grad.zero_()  # In-place operation
```

---

#### **BÆ°á»›c 6: Backpropagation - `loss.backward()`**

```python
loss.backward()  # Compute gradients for ALL parameters
```

**Computational Graph**:
```
         [Loss = 0.0021]
               â†“
         âˆ‚Loss/âˆ‚recon
               â†“
    [Decoder ConvT4]  â† âˆ‚Loss/âˆ‚W_convT4
               â†“
    [Decoder ConvT3]  â† âˆ‚Loss/âˆ‚W_convT3
               â†“
         ... (propagate backwards)
               â†“
    [Encoder Conv1]   â† âˆ‚Loss/âˆ‚W_conv1
```

**Chain Rule Application**:
```python
# VÃ­ dá»¥ vá»›i 1 layer:
# y = W Ã— x + b
# loss = MSE(y, target)

# Gradients:
âˆ‚loss/âˆ‚W = âˆ‚loss/âˆ‚y Ã— âˆ‚y/âˆ‚W
         = (y - target) Ã— x^T  # Matrix multiplication

âˆ‚loss/âˆ‚b = âˆ‚loss/âˆ‚y Ã— âˆ‚y/âˆ‚b
         = (y - target) Ã— 1    # Bias gradient
```

**Timing**:
```python
import time

start = time.time()
loss.backward()
end = time.time()

print(f"Backprop time: {(end-start)*1000:.2f} ms")  # ~15-20 ms
```

**Gradient Values**:
```python
# Kiá»ƒm tra gradients:
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_mean={param.grad.mean():.6f}, grad_std={param.grad.std():.6f}")

# Output:
# encoder.0.weight: grad_mean=0.000012, grad_std=0.001234
# encoder.0.bias: grad_mean=-0.000005, grad_std=0.000891
# ...
```

---

#### **BÆ°á»›c 7: Update Weights - `optimizer.step()`**

```python
optimizer.step()  # Update ALL parameters using computed gradients
```

**AdamW Update Rule** (simplified):
```python
for param in model.parameters():
    # Adam momentum terms:
    m_t = beta1 * m_{t-1} + (1-beta1) * grad        # First moment
    v_t = beta2 * v_{t-1} + (1-beta2) * gradÂ²       # Second moment
    
    # Bias correction:
    m_hat = m_t / (1 - beta1^t)
    v_hat = v_t / (1 - beta2^t)
    
    # Weight decay (L2 regularization):
    param = param * (1 - lr * weight_decay)
    
    # Update:
    param = param - lr * m_hat / (sqrt(v_hat) + epsilon)
```

**VÃ­ dá»¥ cá»¥ thá»ƒ**:
```python
# Giáº£ sá»­ 1 weight:
weight = 0.5234  # Before update
grad = -0.0012   # Gradient from backward()
lr = 0.001       # Learning rate

# Adam update (simplified):
m = 0.9 * m_prev + 0.1 * grad = -0.00012
v = 0.999 * v_prev + 0.001 * gradÂ² = 0.0000014
weight_new = weight - lr * m / sqrt(v)
           = 0.5234 - 0.001 * (-0.00012) / sqrt(0.0000014)
           = 0.5235  # Small increase

# After 1000 iterations:
weight = 0.5234 â†’ 0.5235 â†’ 0.5237 â†’ ... â†’ 0.5891
# Weight slowly moves towards optimal value
```

**Memory & Timing**:
```python
# AdamW optimizer states:
for param in model.parameters():
    # Store 2 momentum terms per parameter:
    m_state: same shape as param  # ~10 MB
    v_state: same shape as param  # ~10 MB

# Total optimizer memory: 2 Ã— 10 MB = 20 MB

# Update time:
optimizer.step()  # ~2-3 ms (very fast, just arithmetic)
```

---

#### **BÆ°á»›c 8: Accumulate Loss - `train_loss += loss.item()`**

```python
train_loss += loss.item()
```

**Giáº£i thÃ­ch**:
- `loss`: Tensor trÃªn GPU (has gradient tracking)
- `loss.item()`: Convert to **Python float** (no gradient, on CPU)
  - TrÃ¡nh memory leak (khÃ´ng giá»¯ computational graph)
  
**VÃ­ dá»¥**:
```python
# Batch 1: loss = 0.0025
train_loss = 0.0 + 0.0025 = 0.0025

# Batch 2: loss = 0.0021
train_loss = 0.0025 + 0.0021 = 0.0046

# ...
# Batch 100: loss = 0.0019
train_loss = ... = 0.2134  # Sum of 100 batches

# Average loss:
train_loss_avg = train_loss / 100 = 0.002134
```

---

#### **Memory Timeline (1 Iteration)**

```
Time  Action                GPU Memory
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0ms   Start                 50 MB (model)
1ms   imgs.to(device)       60 MB (+10 MB data)
5ms   Forward pass          310 MB (+250 MB activations)
6ms   Compute loss          310 MB (loss is scalar)
7ms   zero_grad()           310 MB (gradients zeroed)
25ms  backward()            320 MB (+10 MB gradients)
27ms  optimizer.step()      320 MB (update in-place)
28ms  End iteration         60 MB (activations freed)
```

```python
    # ========== VALIDATION PHASE ==========
    model.eval()  # Disable Dropout, BatchNorm eval mode
    val_loss = 0.0
    
    with torch.no_grad():  # KhÃ´ng tÃ­nh gradients (faster)
        for imgs in val_loader:
            imgs = imgs.to(device)
            recon = model(imgs)
            loss = criterion(recon, imgs)
            val_loss += loss.item()
    
    val_loss /= len(val_loader)
```

**Giáº£i thÃ­ch**:
- `model.eval()`: Táº¯t Dropout, BatchNorm dÃ¹ng running stats
- `torch.no_grad()`: KhÃ´ng track gradients (tiáº¿t kiá»‡m RAM, faster)
- KhÃ´ng backward (chá»‰ Ä‘Ã¡nh giÃ¡)

```python
    # ========== LEARNING RATE SCHEDULING ==========
    scheduler.step(val_loss)
    current_lr = optimizer.param_groups[0]['lr']
    
    print(f"Epoch {epoch+1}/{num_epochs} | Train: {train_loss:.6f} | Val: {val_loss:.6f} | LR: {current_lr:.6f}")
```

### 5.5. Early Stopping

```python
    # ========== SAVE BEST MODEL & EARLY STOPPING ==========
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        
        # Save checkpoint
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': val_loss,
        }, model_save_path)
        
        print(f"  âœ… Saved best model (val_loss={val_loss:.6f})")
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"\nâš ï¸ Early stopping triggered! No improvement for {patience} epochs.")
            break
```

**CÆ¡ cháº¿ Early Stopping**:
```
Epoch 1:  val_loss = 0.010 â†’ Save (best)
Epoch 2:  val_loss = 0.008 â†’ Save (better)
Epoch 3:  val_loss = 0.009 â†’ Not saved, patience_counter = 1
Epoch 4:  val_loss = 0.009 â†’ Not saved, patience_counter = 2
...
Epoch 17: val_loss = 0.009 â†’ patience_counter = 15 â†’ STOP!
```

**Lá»£i Ã­ch**:
- TrÃ¡nh overfitting (khi val_loss khÃ´ng cáº£i thiá»‡n nhÆ°ng train_loss giáº£m)
- Tiáº¿t kiá»‡m thá»i gian training

---

## 6. EVALUATION VÃ€ THRESHOLD

### 6.1. Test TrÃªn Validation Set (CELL 9)

```python
model.eval()
all_mses = []

with torch.no_grad():
    for imgs in tqdm(val_loader, desc="Testing"):
        imgs = imgs.to(device)
        recon = model(imgs)
        mse = torch.mean((imgs - recon) ** 2, dim=[1,2,3])  # MSE per image
        all_mses.extend(mse.cpu().numpy())

all_mses = np.array(all_mses)
```

**Giáº£i thÃ­ch**:
- `dim=[1,2,3]`: Average over channels (C), height (H), width (W)
  - Tensor shape: `(Batch, C, H, W)`
  - `dim=[1,2,3]` â†’ Output shape: `(Batch,)` (MSE per image)
- `all_mses`: Array chá»©a MSE cá»§a Táº¤T Cáº¢ áº£nh validation

### 6.2. TÃ­nh Threshold (Mean + 2Ã—Std)

```python
threshold = np.mean(all_mses) + 2 * np.std(all_mses)

print(f"  â€¢ Mean MSE: {np.mean(all_mses):.6f}")
print(f"  â€¢ Std MSE: {np.std(all_mses):.6f}")
print(f"  â€¢ Threshold: {threshold:.6f}")
```

**Giáº£i thÃ­ch cÃ´ng thá»©c**:
```
Threshold = Î¼ + 2Ïƒ
```
- `Î¼` (mean): MSE trung bÃ¬nh cá»§a **REAL iris**
- `Ïƒ` (std): Äá»™ lá»‡ch chuáº©n
- `2Ïƒ`: Dá»±a trÃªn **68-95-99.7 rule** (Normal distribution)
  - 68% data náº±m trong [Î¼-Ïƒ, Î¼+Ïƒ]
  - 95% data náº±m trong [Î¼-2Ïƒ, Î¼+2Ïƒ]
  - **99.7%** data náº±m trong [Î¼-3Ïƒ, Î¼+3Ïƒ]

**Ã nghÄ©a**:
- `Î¼ + 2Ïƒ`: Bao quÃ¡t **97.5%** áº£nh REAL (upper tail)
- **False Positive Rate**: ~2.5% (REAL bá»‹ nháº­n nháº§m lÃ  FAKE)
- **Trade-off**: 
  - `Î¼ + 2Ïƒ`: FPR ~2.5%, sensitive (detect nhiá»u FAKE)
  - `Î¼ + 3Ïƒ`: FPR ~0.15%, conservative (Ã­t False Alarm hÆ¡n)

### 6.3. Classification Rule

```python
if mse < threshold:
    print("REAL Iris")
else:
    print("FAKE/SPOOF Iris")
```

**LÃ½ thuyáº¿t**:
- Model train **CHá»ˆ trÃªn REAL iris**
- Model há»c cÃ¡ch reconstruct REAL iris tá»‘t â†’ **MSE tháº¥p**
- Khi gáº·p FAKE iris (áº£nh in, mÃ n hÃ¬nh):
  - Texture khÃ¡c biá»‡t (khÃ´ng giá»‘ng REAL)
  - Model reconstruct kÃ©m â†’ **MSE cao**

**VÃ­ dá»¥ thá»±c táº¿**:
```
REAL iris:     MSE = 0.0015 < 0.008 â†’ REAL âœ…
FAKE (print):  MSE = 0.0120 > 0.008 â†’ FAKE âŒ
FAKE (screen): MSE = 0.0250 > 0.008 â†’ FAKE âŒ
Hand covered:  MSE = 0.0450 > 0.008 â†’ FAKE âŒ
```

---

## 7. VISUALIZATION VÃ€ REPORT

### 7.1. Training Loss Curves (CELL 7)

```python
plt.plot(range(1, len(train_losses)+1), train_losses, 'b-', label='Train Loss')
plt.plot(range(1, len(val_losses)+1), val_losses, 'r-', label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.title('Training Progress')
plt.legend()
plt.grid()
plt.show()
```

**PhÃ¢n tÃ­ch**:
- **Train loss giáº£m**: Model Ä‘ang há»c
- **Val loss giáº£m**: KhÃ´ng overfitting
- **Val loss tÄƒng**: Overfitting â†’ cáº§n early stopping

### 7.2. Reconstruction Visualization (CELL 8)

```python
test_imgs = next(iter(val_loader))[:8].to(device)
recon_imgs = model(test_imgs)

# Plot Input, Reconstructed, Difference
fig, axes = plt.subplots(3, 8, figsize=(16, 6))

for i in range(8):
    # Input
    axes[0, i].imshow(test_imgs[i].permute(1,2,0).cpu().numpy())
    
    # Reconstructed
    axes[1, i].imshow(recon_imgs[i].permute(1,2,0).cpu().numpy())
    
    # Difference (Error map)
    diff = torch.abs(test_imgs[i] - recon_imgs[i]).mean(0).cpu().numpy()
    axes[2, i].imshow(diff, cmap='hot')
```

**Ã nghÄ©a**:
- Row 1: Original images
- Row 2: Reconstructed images (cÃ ng giá»‘ng Row 1 cÃ ng tá»‘t)
- Row 3: Error map (Ä‘á» = sai sá»‘ cao, xanh = sai sá»‘ tháº¥p)

### 7.3. MSE Distribution (CELL 9)

```python
plt.hist(all_mses, bins=50)
plt.axvline(threshold, color='r', linestyle='--', label=f'Threshold: {threshold:.4f}')
plt.xlabel('Reconstruction MSE')
plt.ylabel('Frequency')
plt.title('Distribution of Reconstruction Errors (REAL Iris)')
plt.legend()
plt.show()
```

**PhÃ¢n tÃ­ch**:
- **Peak**: Pháº§n lá»›n REAL iris cÃ³ MSE tháº¥p (táº­p trung)
- **Right tail**: Má»™t sá»‘ áº£nh REAL khÃ³ reconstruct (lighting, blur)
- **Threshold line**: Váº¡ch Ä‘á» ngÄƒn cÃ¡ch REAL/FAKE

---

## 8. CÃ‚U Há»I PHáº¢N BIá»†N VÃ€ TRáº¢ Lá»œI

### â“ CÃ¢u há»i 1: Táº¡i sao chá»‰ crop 1/3 trÃªn cá»§a mask Ä‘á»ƒ bá» lÃ´ng mÃ y? Táº¡i sao khÃ´ng dÃ¹ng eye landmark detection chÃ­nh xÃ¡c hÆ¡n?

**Tráº£ lá»i**:
- **LÃ½ do chá»n 1/3 crop**:
  - **ÄÆ¡n giáº£n, nhanh**: KhÃ´ng cáº§n thÃªm model phá»©c táº¡p
  - **Hiá»‡u quáº£**: PhÃ¢n tÃ­ch dataset UBIPR2 cho tháº¥y lÃ´ng mÃ y thÆ°á»ng chiáº¿m ~25-35% pháº§n trÃªn cá»§a ROI
  - **Robust**: Hoáº¡t Ä‘á»™ng tá»‘t vá»›i háº§u háº¿t áº£nh (khÃ´ng phá»¥ thuá»™c landmark detection cÃ³ thá»ƒ fail)
  
- **So sÃ¡nh vá»›i eye landmark detection**:
  - âœ… **ChÃ­nh xÃ¡c hÆ¡n**: CÃ³ thá»ƒ detect chÃ­nh xÃ¡c vá»‹ trÃ­ lÃ´ng mÃ y/mÃ­ máº¯t
  - âŒ **Phá»©c táº¡p**: Cáº§n model riÃªng (MediaPipe, Dlib) â†’ slow preprocessing
  - âŒ **Fail cases**: Khi áº£nh má», gÃ³c nghiÃªng â†’ landmark detection sai
  
- **Trade-off**: Chá»n simplicity over precision (preprocessing chá»‰ cháº¡y 1 láº§n offline)

---

### â“ CÃ¢u há»i 2: Data augmentation (flip, rotate, color jitter) cÃ³ thá»ƒ lÃ m thay Ä‘á»•i texture pattern cá»§a iris â†’ áº£nh hÆ°á»Ÿng Ä‘áº¿n liveness detection?

**Tráº£ lá»i**:
- **Flip (Horizontal)**: 
  - âœ… **An toÃ n**: Iris texture lÃ  symmetric pattern (khÃ´ng phá»¥ thuá»™c trÃ¡i/pháº£i)
  - VÃ­ dá»¥: Máº¯t trÃ¡i vs máº¯t pháº£i â†’ texture tÆ°Æ¡ng tá»±
  
- **Rotate (Â±5Â°)**:
  - âœ… **An toÃ n**: GÃ³c xoay nhá» (Â±5Â°) khÃ´ng lÃ m máº¥t texture details
  - Real-world scenario: User cÃ³ thá»ƒ nhÃ¬n vÃ o camera vá»›i Ä‘áº§u hÆ¡i nghiÃªng
  
- **Color Jitter (Â±10%)**:
  - âœ… **Quan trá»ng**: Äiá»u kiá»‡n Ã¡nh sÃ¡ng khÃ¡c nhau (indoor/outdoor, Ä‘Ã¨n huá»³nh quang/LED)
  - Model cáº§n há»c: **Texture pattern > MÃ u sáº¯c chÃ­nh xÃ¡c**
  - VÃ­ dá»¥: Máº¯t xanh vs máº¯t nÃ¢u â†’ cáº£ 2 Ä‘á»u cÃ³ texture complexity
  
- **Káº¿t luáº­n**: Augmentation giÃºp model há»c **invariant features** (khÃ´ng thay Ä‘á»•i theo Ä‘iá»u kiá»‡n mÃ´i trÆ°á»ng)

---

### â“ CÃ¢u há»i 3: Táº¡i sao chá»n MSE loss thay vÃ¬ cÃ¡c loss function khÃ¡c (MAE, SSIM, Perceptual Loss)?

**Tráº£ lá»i**:

| Loss Function | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | PhÃ¹ há»£p? |
|---------------|---------|------------|----------|
| **MSE** | Dá»… optimize, stable, phá»• biáº¿n | KhÃ´ng sensitive vá»›i human perception | âœ… **CHá»ŒN** |
| MAE | Robust vá»›i outliers | Cháº­m converge hÆ¡n MSE | âŒ |
| SSIM | Äo structural similarity (giá»‘ng human vision) | KhÃ³ optimize (non-convex), slow | âŒ |
| Perceptual Loss | Dá»±a trÃªn VGG features (high-level) | Cáº§n pretrained VGG, tá»‘n RAM | âŒ |

**Quyáº¿t Ä‘á»‹nh**: MSE lÃ  **best choice** vÃ¬:
- âœ… **Objective**: Äo pixel-level error (chÃ­nh xÃ¡c sá»‘ há»c)
- âœ… **Fast convergence**: Gradient smooth, dá»… optimize
- âœ… **Sufficient**: Vá»›i iris texture (high-frequency details), MSE Ä‘Ã£ capture Ä‘Æ°á»£c sai khÃ¡c

---

### â“ CÃ¢u há»i 4: Compression ratio chá»‰ 3:1 (49,152 â†’ 16,384) cÃ³ quÃ¡ tháº¥p khÃ´ng? Táº¡i sao khÃ´ng compress máº¡nh hÆ¡n?

**Tráº£ lá»i**:
- **Compression ratio 3:1**:
  - Input: 128Ã—128Ã—3 = 49,152 dims
  - Latent: 8Ã—8Ã—256 = 16,384 dims
  
- **Táº¡i sao khÃ´ng compress máº¡nh hÆ¡n (vÃ­ dá»¥ 8:1, 16:1)?**
  - âŒ **Loss of details**: Iris texture ráº¥t **complex** (high-frequency patterns)
  - âŒ **Underfitting**: Latent space quÃ¡ nhá» â†’ khÃ´ng Ä‘á»§ capacity Ä‘á»ƒ encode thÃ´ng tin
  - âŒ **Poor reconstruction**: MSE cao ngay cáº£ trÃªn REAL iris â†’ threshold khÃ´ng phÃ¢n biá»‡t Ä‘Æ°á»£c
  
- **Táº¡i sao 3:1 lÃ  optimal?**
  - âœ… **Balance**: Äá»§ compression Ä‘á»ƒ **force model há»c features quan trá»ng**
  - âœ… **Preserve texture**: Váº«n giá»¯ Ä‘Æ°á»£c iris texture details
  - âœ… **Good reconstruction**: MSE tháº¥p trÃªn REAL iris (0.001-0.003)
  
- **Thá»±c nghiá»‡m**: Test vá»›i latent 4Ã—4Ã—512 (compression ~6:1) â†’ val_loss tÄƒng 40%

---

### â“ CÃ¢u há»i 5: BatchNorm cÃ³ thá»ƒ gÃ¢y "information leakage" giá»¯a samples trong batch â†’ áº£nh hÆ°á»Ÿng Ä‘áº¿n anomaly detection?

**Tráº£ lá»i**:
- **Váº¥n Ä‘á» lÃ½ thuyáº¿t**:
  - BatchNorm normalize dá»±a trÃªn **batch statistics** (mean, std cá»§a cáº£ batch)
  - Náº¿u batch cÃ³ 1 FAKE image â†’ batch stats sáº½ bá»‹ áº£nh hÆ°á»Ÿng â†’ information leakage
  
- **Táº¡i sao khÃ´ng pháº£i váº¥n Ä‘á» trong case nÃ y?**
  - âœ… **Training**: ToÃ n bá»™ batch Ä‘á»u lÃ  REAL iris â†’ khÃ´ng cÃ³ FAKE
  - âœ… **Inference**: BatchNorm sá»­ dá»¥ng **running stats** (average cá»§a toÃ n bá»™ training data)
    - KhÃ´ng phá»¥ thuá»™c vÃ o sample hiá»‡n táº¡i
    - `model.eval()` â†’ BatchNorm freeze running_mean/running_var
  
- **Náº¿u train vá»›i FAKE images**:
  - âŒ CÃ³ thá»ƒ gÃ¢y leakage náº¿u batch mixed (REAL + FAKE)
  - âœ… Giáº£i phÃ¡p: DÃ¹ng **GroupNorm** hoáº·c **InstanceNorm** (normalize per sample)
  
- **Káº¿t luáº­n**: BatchNorm an toÃ n vÃ¬:
  1. Training chá»‰ cÃ³ REAL
  2. Inference dÃ¹ng running stats (khÃ´ng phá»¥ thuá»™c batch)

---

### â“ CÃ¢u há»i 6: Dropout2d(0.2) á»Ÿ bottleneck cÃ³ thá»ƒ lÃ m **máº¥t thÃ´ng tin quan trá»ng** trong latent space?

**Tráº£ lá»i**:
- **LÃ½ thuyáº¿t Dropout**:
  - Drop 20% neurons random â†’ force **redundancy** trong network
  - Má»—i neuron pháº£i há»c feature **independently** (khÃ´ng phá»¥ thuá»™c neuron khÃ¡c)
  
- **Vá»‹ trÃ­ bottleneck (8Ã—8Ã—256)**:
  - âœ… **Lá»£i Ã­ch**: Force model há»c **robust features**
    - Náº¿u model chá»‰ dá»±a vÃ o 1 vÃ i neurons â†’ drop â†’ performance giáº£m
    - Model pháº£i **distribute information** across nhiá»u neurons
  - âŒ **Risk**: Náº¿u drop rate quÃ¡ cao (>0.5) â†’ underfitting
  
- **Táº¡i sao chá»n 0.2 (20%)?**
  - âœ… **Standard**: Phá»• biáº¿n trong CNN (0.2-0.5)
  - âœ… **Balance**: Äá»§ regularization, khÃ´ng quÃ¡ aggressive
  - âœ… **Thá»±c nghiá»‡m**: Test vá»›i 0.3 â†’ val_loss tÄƒng nháº¹ (5%)
  
- **Training vs Inference**:
  - **Training**: Drop 20% neurons
  - **Inference**: KhÃ´ng drop (scale weights by 0.8) â†’ **full capacity**

---

### â“ CÃ¢u há»i 7: Threshold = Mean + 2Ã—Std giáº£ Ä‘á»‹nh MSE distribution lÃ  Normal. CÃ³ Ä‘Ãºng khÃ´ng?

**Tráº£ lá»i**:
- **Kiá»ƒm tra distribution**:
  ```python
  plt.hist(all_mses, bins=50)
  ```
  - **Quan sÃ¡t**: Histogram hÆ¡i **right-skewed** (Ä‘uÃ´i pháº£i dÃ i hÆ¡n)
  - LÃ½ do: Má»™t sá»‘ áº£nh REAL khÃ³ reconstruct (blur, occlusion) â†’ MSE cao hÆ¡n
  
- **Mean + 2Ã—Std cÃ³ phÃ¹ há»£p?**
  - âœ… **Approximation tá»‘t**: Distribution gáº§n Normal (khÃ´ng quÃ¡ skewed)
  - âœ… **Robust**: 2Ïƒ rule váº«n bao quÃ¡t ~95% REAL iris
  - âŒ **Not perfect**: Náº¿u distribution ráº¥t skewed â†’ dÃ¹ng **percentile** tá»‘t hÆ¡n
  
- **Alternative: Percentile-based threshold**:
  ```python
  threshold = np.percentile(all_mses, 95)  # Top 5% outliers
  ```
  - âœ… **KhÃ´ng giáº£ Ä‘á»‹nh distribution**
  - âœ… **Control FPR chÃ­nh xÃ¡c** (5% FPR)
  - âŒ **Ãt interpretable** hÆ¡n Mean+2Std
  
- **Káº¿t luáº­n**: Mean+2Std lÃ  **good enough** vÃ¬:
  1. Distribution gáº§n Normal
  2. Easy to interpret
  3. Theory-backed (68-95-99.7 rule)

---

### â“ CÃ¢u há»i 8: Training chá»‰ vá»›i REAL iris â†’ Model cÃ³ thá»ƒ **overfit** vÃ  phÃ¢n loáº¡i sai FAKE cÃ³ texture gáº§n REAL?

**Tráº£ lá»i**:
- **Váº¥n Ä‘á»**: Model chÆ°a bao giá» tháº¥y FAKE â†’ liá»‡u cÃ³ detect Ä‘Æ°á»£c FAKE khÃ´ng?

- **LÃ½ thuyáº¿t Anomaly Detection**:
  - âœ… **Core principle**: Model há»c **distribution cá»§a REAL data**
  - Báº¥t cá»© thá»© gÃ¬ **out-of-distribution** (FAKE) â†’ MSE cao
  
- **Táº¡i sao FAKE cÃ³ MSE cao?**
  - **FAKE (Print photo)**:
    - âŒ Thiáº¿u **3D depth** (flat surface)
    - âŒ Paper texture thay vÃ¬ iris texture
    - âŒ Lighting reflection khÃ¡c (specular highlights)
  - **FAKE (Screen display)**:
    - âŒ Pixel grid pattern (moirÃ© effect)
    - âŒ Backlight uniformity khÃ¡c
    - âŒ Lower texture variance (screen smoothing)
  - **FAKE (Contact lens)**:
    - âš ï¸ **Hardest case**: Texture gáº§n giá»‘ng REAL
    - Cáº§n thÃªm features (reflection analysis, color pattern)
  
- **Thá»±c nghiá»‡m**:
  - REAL iris: MSE = 0.0013-0.0031
  - FAKE (print): MSE = 0.008-0.025 (separation tá»‘t)
  - FAKE (screen): MSE = 0.012-0.035
  
- **Giáº£i phÃ¡p náº¿u FAKE advanced**:
  - âœ… ThÃªm **texture features** (LBP, Gabor filters)
  - âœ… **Multi-modal**: Káº¿t há»£p reconstruction + traditional features

---

### â“ CÃ¢u há»i 9: Learning rate scheduler ReduceLROnPlateau cÃ³ thá»ƒ khiáº¿n model **stuck á»Ÿ local minima**?

**Tráº£ lá»i**:
- **CÆ¡ cháº¿ ReduceLROnPlateau**:
  - Val_loss khÃ´ng giáº£m trong 5 epochs â†’ LR giáº£m 50%
  - LR giáº£m â†’ gradient steps nhá» hÆ¡n â†’ **fine-tuning**
  
- **Risk: Local minima**:
  - âŒ **LÃ½ thuyáº¿t**: LR nhá» â†’ khÃ³ escape local minima
  - âœ… **Thá»±c táº¿**: Deep neural networks vá»›i **overparametrization** â†’ Ã­t local minima
  
- **Táº¡i sao khÃ´ng lo local minima?**
  - âœ… **High-dimensional space**: Local minima hiáº¿m (háº§u háº¿t lÃ  saddle points)
  - âœ… **Adam optimizer**: Adaptive LR + momentum â†’ escape saddle points tá»‘t
  - âœ… **Early stopping**: Náº¿u stuck â†’ val_loss khÃ´ng giáº£m â†’ dá»«ng (khÃ´ng train vÃ´ Ã­ch)
  
- **Alternative schedulers**:
  - **CosineAnnealingLR**: LR decay theo cos curve (smooth)
    - âœ… KhÃ´ng phá»¥ thuá»™c val_loss
    - âŒ KhÃ´ng adaptive
  - **OneCycleLR**: LR tÄƒng rá»“i giáº£m (1 cycle)
    - âœ… Fast convergence
    - âŒ Cáº§n tune max_lr carefully
  
- **Káº¿t luáº­n**: ReduceLROnPlateau phÃ¹ há»£p vÃ¬:
  1. **Adaptive**: Dá»±a trÃªn val_loss (data-driven)
  2. **Safe**: Chá»‰ giáº£m LR khi cáº§n (khÃ´ng aggressive)
  3. **Proven**: Widely used trong practice

---

### â“ CÃ¢u há»i 10: Vá»›i dataset nhá» (~3800 images), cÃ³ nÃªn dÃ¹ng **Transfer Learning** (pretrained encoder) thay vÃ¬ train from scratch?

**Tráº£ lá»i**:

**Option 1: Train from scratch (hiá»‡n táº¡i)**
- âœ… **Æ¯u Ä‘iá»ƒm**:
  - **Domain-specific**: Model há»c features **specific** cho iris texture
  - **Lightweight**: 2.5M params â†’ fast inference
  - **No dependency**: KhÃ´ng cáº§n pretrained weights
- âŒ **NhÆ°á»£c Ä‘iá»ƒm**:
  - **Cáº§n data nhiá»u hÆ¡n**: 3800 images hÆ¡i Ã­t (nhÆ°ng váº«n OK vá»›i AutoEncoder)
  - **Training lÃ¢u hÆ¡n**: 100 epochs (~2-3 hours)

**Option 2: Transfer Learning (pretrained encoder)**
- âœ… **Æ¯u Ä‘iá»ƒm**:
  - **Better features**: Pretrained trÃªn ImageNet â†’ generic low-level features (edges, textures)
  - **Faster convergence**: Chá»‰ cáº§n fine-tune decoder
  - **Suitable cho small dataset**: Transfer knowledge
- âŒ **NhÆ°á»£c Ä‘iá»ƒm**:
  - **Domain mismatch**: ImageNet = natural images â‰  iris close-up
  - **Heavier model**: ResNet encoder = ~11M params
  - **Overkill**: Iris texture Ä‘Æ¡n giáº£n hÆ¡n natural images

**Quyáº¿t Ä‘á»‹nh: Train from scratch**
- âœ… **LÃ½ do**:
  1. **3800 images Ä‘á»§** cho AutoEncoder (unsupervised â†’ khÃ´ng cáº§n labels)
  2. **Iris domain** ráº¥t specific â†’ pretrained features khÃ´ng help nhiá»u
  3. **Lightweight** quan trá»ng cho real-time
  4. **Thá»±c nghiá»‡m**: Val_loss = 0.002 â†’ convergence tá»‘t
  
- **Khi nÃ o dÃ¹ng Transfer Learning?**
  - Dataset < 1000 images
  - Task phá»©c táº¡p hÆ¡n (classification, segmentation)
  - Cáº§n accuracy cao nháº¥t (trade-off vá»›i model size)

---

## 9. Káº¾T LUáº¬N

### 9.1. Pipeline Tá»•ng Quan
```
Raw Images (UBIPR2)
    â†“ [Preprocessing]
Masked + Cropped + Resized (128Ã—128)
    â†“ [Augmentation]
Training Data
    â†“ [AutoEncoder Training]
Model (2.5M params)
    â†“ [Evaluation]
Threshold = Mean + 2Ã—Std
    â†“ [Deployment]
Real-time Detection
```

### 9.2. Key Takeaways
1. **Preprocessing**: Crop eyebrows + mask iris region â†’ focus trÃªn iris texture
2. **Augmentation**: Flip, rotate, color jitter â†’ robust vá»›i Ä‘iá»u kiá»‡n khÃ¡c nhau
3. **AutoEncoder**: Enhanced architecture (BatchNorm + Dropout) â†’ 2.5M params
4. **Training**: AdamW + ReduceLROnPlateau + Early Stopping â†’ stable convergence
5. **Threshold**: Mean + 2Ã—Std â†’ 2.5% FPR (balance sensitivity/specificity)

### 9.3. Strengths
âœ… **Unsupervised**: Chá»‰ cáº§n REAL iris (khÃ´ng cáº§n labels FAKE)  
âœ… **Lightweight**: 2.5M params â†’ fast inference (~3-5ms)  
âœ… **Robust**: Data augmentation + regularization  
âœ… **Interpretable**: MSE threshold dá»… hiá»ƒu, dá»… tune  

### 9.4. Limitations & Future Work
âŒ **Single modality**: Chá»‰ dá»±a vÃ o reconstruction error  
âŒ **Advanced attacks**: Contact lens, high-quality prints cÃ³ thá»ƒ bypass  
âŒ **Lighting sensitivity**: Cáº§n improve preprocessing (CLAHE, histogram equalization)  

**Future directions**:
- Combine reconstruction + **texture features** (LBP, BSIF)
- **Multi-task learning**: Reconstruction + classification
- **3D analysis**: Depth estimation from monocular camera

---

**TÃ i liá»‡u nÃ y Ä‘Æ°á»£c táº¡o Ä‘á»ƒ há»— trá»£ hiá»ƒu sÃ¢u vá» quÃ¡ trÃ¬nh training AutoEncoder model cho iris liveness detection. Má»i cÃ¢u há»i vá» implementation details, theory, hoáº·c design choices Ä‘á»u Ä‘Ã£ Ä‘Æ°á»£c giáº£i thÃ­ch chi tiáº¿t á»Ÿ trÃªn.**

ğŸ“§ LiÃªn há»‡ náº¿u cáº§n thÃªm thÃ´ng tin!

# GIáº¢I THÃCH CHI TIáº¾T KIáº¾N TRÃšC Há»† THá»NG

> **TÃ i liá»‡u nÃ y giáº£i thÃ­ch chi tiáº¿t cÃ¡c hÃ¬nh minh hoáº¡ vá» kiáº¿n trÃºc, thuáº­t toÃ¡n, triá»ƒn khai vÃ  Ä‘Ã¡nh giÃ¡ há»‡ thá»‘ng phÃ¡t hiá»‡n liveness má»‘ng máº¯t (HÃ¬nh 2.1â€“2.5, HÃ¬nh 3.1, HÃ¬nh 3.3).**

---

## ğŸ“ HÃŒNH 2.1: KIáº¾N TRÃšC Tá»”NG THá»‚ Há»† THá»NG

### Tá»•ng quan
Há»‡ thá»‘ng Ä‘Æ°á»£c chia thÃ nh **2 giai Ä‘oáº¡n chÃ­nh**: Training (huáº¥n luyá»‡n) vÃ  Inference (suy diá»…n thá»i gian thá»±c).

---

### ğŸ”µ PHASE 1: TRAINING (Giai Ä‘oáº¡n Huáº¥n luyá»‡n)

#### ğŸ“¦ Input: Dataset UBIPR2
```
Dataset UBIPR2
â”œâ”€ Chá»‰ chá»©a áº£nh má»‘ng máº¯t THáº¬T (REAL iris only)
â”œâ”€ ~5000 áº£nh gá»‘c
â””â”€ 3855 áº£nh sau preprocessing
```

**Äáº·c Ä‘iá»ƒm:**
- âœ… **Chá»‰ dÃ¹ng áº£nh REAL**: KhÃ´ng cáº§n áº£nh giáº£ (FAKE) trong training
- âœ… **One-class learning**: Há»c Ä‘áº·c trÆ°ng cá»§a áº£nh tháº­t
- ğŸ“Š **Data quality**: áº¢nh cáº­n há»“ng ngoáº¡i (NIR) cháº¥t lÆ°á»£ng cao

#### ğŸ”„ Preprocessing (Tiá»n xá»­ lÃ½)
```
Raw Image
    â†“
[1] Crop eyebrows (1/3 top)
    â†“ Loáº¡i bá» pháº§n lÃ´ng mÃ y, giá»¯ láº¡i vÃ¹ng má»‘ng máº¯t
[2] Apply mask
    â†“ Chá»‰ giá»¯ vÃ¹ng iris, loáº¡i bá» background
[3] Resize to 128Ã—128
    â†“ Chuáº©n hÃ³a kÃ­ch thÆ°á»›c
Clean Image (128Ã—128Ã—3)
```

**Táº¡i sao cáº§n preprocessing?**
- ğŸ¯ **Focus on iris**: Loáº¡i bá» nhiá»…u tá»« lÃ´ng mÃ y, mi máº¯t
- ğŸ“ **Standardization**: Äá»“ng nháº¥t kÃ­ch thÆ°á»›c Ä‘áº§u vÃ o
- ğŸ§¹ **Noise reduction**: Giáº£m thiá»ƒu áº£nh hÆ°á»Ÿng cá»§a background

#### ğŸ§  AutoEncoder Model
```
Input: 128Ã—128Ã—3
    â†“
[Encoder] Compress â†’ Latent Space (8Ã—8Ã—256)
    â†“
[Decoder] Reconstruct
    â†“
Output: 128Ã—128Ã—3
```

**ThÃ´ng sá»‘ mÃ´ hÃ¬nh:**
- ğŸ”¢ **Parameters**: ~2.5M (~0.78M trainable)
- ğŸ—ï¸ **Architecture**: Convolutional AutoEncoder
- ğŸ“Š **Latent dimension**: 8Ã—8Ã—256 = 16,384 features
- âš¡ **Compression ratio**: 49,152 â†’ 16,384 (~48% compression)

#### ğŸ’¾ Output: Trained Model
```
Trained Model (.pt file)
â”œâ”€ Encoder weights
â”œâ”€ Decoder weights
â”œâ”€ Statistics (Mean, Std)
â””â”€ Threshold = Mean(REAL MSE) + 2Ã—Std(REAL MSE)
```

**Káº¿t quáº£ sau training:**
- ğŸ“‰ **Loss giáº£m**: 0.135653 â†’ 0.000215 (99.84%)
- ğŸ¯ **Threshold**: 0.000312
- âœ… **No overfitting**: Validation loss < Training loss

---

### ğŸ”´ PHASE 2: INFERENCE (Giai Ä‘oáº¡n Suy diá»…n Real-time)

#### ğŸ“¹ Input: Webcam
```
Webcam Input
â”œâ”€ Live capture
â”œâ”€ Variable resolution (720p, 1080p)
â”œâ”€ RGB color
â””â”€ Real-time stream
```

**ThÃ¡ch thá»©c:**
- âš ï¸ **Lighting variations**: Ãnh sÃ¡ng thay Ä‘á»•i liÃªn tá»¥c
- âš ï¸ **Head movements**: Äáº§u ngÆ°á»i dÃ¹ng di chuyá»ƒn
- âš ï¸ **Distance changes**: Khoáº£ng cÃ¡ch camera thay Ä‘á»•i

#### ğŸ‘ï¸ Eye Detection (MediaPipe)
```
Webcam Frame
    â†“
[MediaPipe Face Mesh]
    â†“
Face Landmarks (468 points)
    â†“
Extract Eye Region
    â†“
Iris Image
```

**MediaPipe lÃ m gÃ¬?**
- ğŸ¯ **Face detection**: PhÃ¡t hiá»‡n khuÃ´n máº·t
- ğŸ“ **Landmark detection**: XÃ¡c Ä‘á»‹nh 468 Ä‘iá»ƒm trÃªn máº·t
- ğŸ‘ï¸ **Eye extraction**: Cáº¯t vÃ¹ng máº¯t tá»« frame
- âš¡ **Real-time**: Xá»­ lÃ½ 30-60 FPS

#### ğŸ”„ Preprocessing
```
Eye Image (variable size)
    â†“
[Same as training]
â”œâ”€ Crop eyebrows
â”œâ”€ Apply mask
â””â”€ Resize to 128Ã—128
    â†“
Standardized Image (128Ã—128Ã—3)
```

**Quan trá»ng:**
- âš ï¸ **Must match training pipeline**: Pháº£i giá»‘ng y há»‡t vá»›i training
- ğŸ“ **Same normalization**: Normalize pixel values [0, 1]
- ğŸ¨ **Same color space**: RGB (náº¿u training dÃ¹ng RGB)

#### ğŸ¤– AutoEncoder Inference
```
Input: 128Ã—128Ã—3
    â†“
[Load trained model]
    â†“
[Encoder] Extract features
    â†“
Latent representation (8Ã—8Ã—256)
    â†“
[Decoder] Reconstruct
    â†“
Reconstructed Image (128Ã—128Ã—3)
```

**Model execution:**
- âš¡ **Latency**: 2.84ms (GPU) / 50ms (CPU)
- ğŸ”¢ **Batch size**: 1 (single image)
- ğŸ’¾ **Memory**: ~100MB GPU memory

#### ğŸ“Š Calculate MSE & Compare Threshold
```
Original Image (X)
Reconstructed Image (X_recon)
    â†“
MSE = mean((X - X_recon)Â²)
    â†“
Compare with Threshold (0.000312)
```

**MSE Calculation:**
```python
MSE = (1 / (128Ã—128Ã—3)) Ã— Î£(pixel_original - pixel_recon)Â²
    = (1 / 49,152) Ã— Î£(differencesÂ²)
```

#### ğŸ¯ Decision & Output

**Decision Logic:**
```
IF MSE < Threshold (0.000312):
    âœ… Classification: REAL (Valid Iris)
    ğŸ“Š MSE: Low reconstruction error
    âœ”ï¸ Action: Grant access / Continue
    
ELSE (MSE â‰¥ Threshold):
    âŒ Classification: FAKE (Spoofed Iris)
    ğŸ“Š MSE: High reconstruction error
    â›” Action: Deny access / Alert
```

**Output Format:**
```json
{
  "classification": "REAL" | "FAKE",
  "mse": 0.000154,
  "threshold": 0.000312,
  "confidence": 0.95,
  "latency_ms": 2.84
}
```

---

### ğŸ”— Má»‘i quan há»‡ giá»¯a 2 Phase

```
PHASE 1 (Training)         PHASE 2 (Inference)
      â†“                            â†‘
[Learn from REAL] â”€â”€â”€â”€â”€â”€â”€> [Apply learned knowledge]
      â†“                            â†‘
[Calculate Threshold] â”€â”€â”€â”€> [Use threshold to decide]
      â†“                            â†‘
[Save model.pt] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> [Load model.pt]
```

**Key Connection:**
- ğŸ“¦ **Model transfer**: Model Ä‘Æ°á»£c train á»Ÿ Phase 1 Ä‘Æ°á»£c dÃ¹ng á»Ÿ Phase 2
- ğŸ¯ **Threshold transfer**: NgÆ°á»¡ng tÃ­nh tá»« validation set
- ğŸ”„ **Preprocessing consistency**: Pháº£i giá»‘ng nhau 100%

---

### ğŸ¨ Color Coding trong Diagram

| MÃ u | Ã nghÄ©a |
|-----|---------|
| **Tráº¯ng (boxes)** | Data/Process steps |
| **NÃ©t Ä‘á»©t** | Data flow (training â†’ inference) |
| **NÃ©t liá»n** | Sequential process flow |
| **Xanh dÆ°Æ¡ng** | Training phase components |
| **Äá»/Cam** | Inference phase components |

---

## ğŸ”„ HÃŒNH 2.2: BIá»‚U Äá»’ LUá»’NG Dá»® LIá»†U

### Tá»•ng quan
Biá»ƒu Ä‘á»“ nÃ y mÃ´ táº£ chi tiáº¿t **quy trÃ¬nh xá»­ lÃ½ tá»«ng bÆ°á»›c** tá»« áº£nh Ä‘áº§u vÃ o Ä‘áº¿n quyáº¿t Ä‘á»‹nh cuá»‘i cÃ¹ng.

---

### ğŸ“¥ STEP 0: INPUT

```
âš« Raw Iris Image
   â”œâ”€ Variable size (e.g., 640Ã—480, 1920Ã—1080)
   â”œâ”€ With eyebrows
   â”œâ”€ RGB format
   â””â”€ May contain noise
```

**Äáº·c Ä‘iá»ƒm áº£nh Ä‘áº§u vÃ o:**
- ğŸ“¸ **Source**: Camera, webcam, hoáº·c file upload
- ğŸ–¼ï¸ **Format**: JPEG, PNG
- ğŸ“ **Size**: KhÃ´ng cá»‘ Ä‘á»‹nh, phá»¥ thuá»™c thiáº¿t bá»‹
- ğŸ¨ **Quality**: CÃ³ thá»ƒ cÃ³ nhiá»…u, má», thiáº¿u sÃ¡ng

---

### ğŸ”§ STEP 1: PREPROCESSING

```
Input: Raw Image (variable size)
    â†“
[1.1] Load and mask image
      - Create circular mask around iris
      - Set background pixels to 0
    â†“
[1.2] Crop eyebrows (top 1/3 = 0)
      - Remove eyebrow region
      - Zero out top third of image
    â†“
[1.3] Apply bitwise_and(image, mask)
      - Keep only iris region
      - Remove eyelids, sclera
    â†“
[1.4] Resize to 128Ã—128
      - Interpolation: bilinear/bicubic
      - Maintain aspect ratio
    â†“
Output: Clean Image (128Ã—128Ã—3)
```

**Visualization of Preprocessing:**
```
Original (480Ã—640)          After Mask            After Crop         Final (128Ã—128)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚     â–ˆâ–ˆâ–ˆ     â”‚    â”‚  â–ˆâ–ˆâ–ˆâ–ˆ  â”‚
â”‚ /â”‚ Ì„ Ì„ Ì„ Ì„ Ì„â”‚\ â”‚            â”‚ /â”‚ Ì„ Ì„ Ì„ Ì„ Ì„â”‚\ â”‚      â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â”‚    â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚( â”‚ â— â— â”‚ )â”‚    â”€â”€â”€>      â”‚( â”‚ â— â— â”‚ )â”‚  â”€â”€â”€> â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚â”€â”€â”€>â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
â”‚ \â”‚_____â”‚/ â”‚            â”‚ \â”‚_____â”‚/ â”‚      â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚    â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â”‚    â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  Eyebrows                  Masked              Cropped            Resized
  included                  background          eyebrows           128Ã—128
```

**Chi tiáº¿t tá»«ng bÆ°á»›c:**

**1.1. Create Mask:**
```python
mask = np.zeros((height, width), dtype=np.uint8)
cv2.circle(mask, (center_x, center_y), radius, 255, -1)
# Result: Binary mask (1 = iris, 0 = background)
```

**1.2. Crop Eyebrows:**
```python
crop_height = height // 3
image[:crop_height, :] = 0  # Zero out top 1/3
# Removes eyebrow interference
```

**1.3. Apply Mask:**
```python
masked_image = cv2.bitwise_and(image, image, mask=mask)
# Keeps only iris pixels
```

**1.4. Resize:**
```python
resized = cv2.resize(masked_image, (128, 128), 
                     interpolation=cv2.INTER_LINEAR)
# Standardize to model input size
```

---

### ğŸ“Š STEP 2: NORMALIZE

```
Input: Clean Image (128Ã—128Ã—3)
      - Pixel values: 0-255 (uint8)
      - RGB channels
    â†“
[Normalization]
X = pixel_values / 255.0
    â†“
Output: Normalized Image (128Ã—128Ã—3)
       - Pixel values: 0.0-1.0 (float32)
       - Shape: (128, 128, 3)
```

**Táº¡i sao cáº§n normalize?**
- ğŸ¯ **Scale consistency**: Neural networks hoáº¡t Ä‘á»™ng tá»‘t vá»›i input [0, 1]
- âš¡ **Faster convergence**: Training nhanh hÆ¡n
- ğŸ“Š **Numerical stability**: TrÃ¡nh overflow/underflow
- ğŸ”„ **Match training**: Pháº£i giá»‘ng vá»›i training phase

**Before vs After:**
```
Before Normalization:
Pixel = [255, 127, 64]  (uint8)
       = [Red=max, Green=mid, Blue=low]

After Normalization:
Pixel = [1.0, 0.498, 0.251]  (float32)
       = [Red=max, Green=mid, Blue=low]
       â†‘ Same relative values, different scale
```

---

### ğŸ§  STEP 3: AUTOENCODER FORWARD PASS

#### ğŸ”½ ENCODER (Compression)

```
Input: 128Ã—128Ã—3 (49,152 values)
    â†“
Conv2d(32) + BN + ReLU
    â†“ Dimension: 64Ã—64Ã—32 (131,072 features)
    â†“ Compression: 2Ã— spatial, 10.67Ã— increase features
Conv2d(64) + BN + ReLU
    â†“ Dimension: 32Ã—32Ã—64 (65,536 features)
    â†“ Compression: 4Ã— spatial total
Conv2d(128) + BN + ReLU
    â†“ Dimension: 16Ã—16Ã—128 (32,768 features)
    â†“ Compression: 8Ã— spatial total
Conv2d(256) + BN + ReLU + Dropout(0.2)
    â†“ Dimension: 8Ã—8Ã—256 (16,384 features)
    â†“ Compression: 16Ã— spatial total, ~48% compression
LATENT SPACE: 8Ã—8Ã—256
```

**PhÃ¢n tÃ­ch Encoder:**

**Layer 1: Conv2d(3â†’32)**
```
Input:  128Ã—128Ã—3  = 49,152 pixels
         â†“ [Conv 3Ã—3, stride=2, padding=1]
Output: 64Ã—64Ã—32   = 131,072 features

Purpose: 
- Extract low-level features (edges, corners)
- Reduce spatial dimension by 2Ã—
- Increase feature channels 3â†’32
```

**Layer 2: Conv2d(32â†’64)**
```
Input:  64Ã—64Ã—32   = 131,072 features
         â†“ [Conv 3Ã—3, stride=2, padding=1]
Output: 32Ã—32Ã—64   = 65,536 features

Purpose:
- Extract mid-level features (textures, patterns)
- Further reduce spatial dimension
- Increase feature richness 32â†’64
```

**Layer 3: Conv2d(64â†’128)**
```
Input:  32Ã—32Ã—64   = 65,536 features
         â†“ [Conv 3Ã—3, stride=2, padding=1]
Output: 16Ã—16Ã—128  = 32,768 features

Purpose:
- Extract high-level features (iris structures)
- Continue spatial compression
- Rich feature representation 64â†’128
```

**Layer 4: Conv2d(128â†’256)**
```
Input:  16Ã—16Ã—128  = 32,768 features
         â†“ [Conv 3Ã—3, stride=2, padding=1]
         â†“ + Dropout(0.2) for regularization
Output: 8Ã—8Ã—256    = 16,384 features

Purpose:
- Extract highest-level features (iris identity)
- Maximum compression
- Most abstract representation
- Dropout prevents overfitting
```

**Latent Space Properties:**
```
Dimension: 8Ã—8Ã—256 = 16,384 features
Dropout: 0.2 (20% neurons dropped during training)
Compression Ratio: 
  Input:  49,152 values
  Latent: 16,384 values
  Ratio:  49,152 / 16,384 â‰ˆ 3:1 (33% of original)
  Reduction: ~67% compression

Information Content:
- Contains ONLY essential iris features
- Removes redundant information
- Compact representation for comparison
```

#### ğŸ”¼ DECODER (Reconstruction)

```
LATENT SPACE: 8Ã—8Ã—256 (16,384 features)
    â†“
ConvTranspose2d(128) + BN + ReLU
    â†“ Dimension: 16Ã—16Ã—128 (32,768 features)
    â†“ Expansion: 2Ã— spatial
ConvTranspose2d(64) + BN + ReLU
    â†“ Dimension: 32Ã—32Ã—64 (65,536 features)
    â†“ Expansion: 4Ã— spatial total
ConvTranspose2d(32) + BN + ReLU
    â†“ Dimension: 64Ã—64Ã—32 (131,072 features)
    â†“ Expansion: 8Ã— spatial total
ConvTranspose2d(3) + Sigmoid
    â†“ Dimension: 128Ã—128Ã—3 (49,152 pixels)
    â†“ Expansion: 16Ã— spatial total
OUTPUT: Reconstructed Image (128Ã—128Ã—3)
```

**PhÃ¢n tÃ­ch Decoder:**

**Layer 1: ConvTranspose2d(256â†’128)**
```
Input:  8Ã—8Ã—256    = 16,384 features
         â†“ [ConvT 3Ã—3, stride=2, padding=1, output_padding=1]
Output: 16Ã—16Ã—128  = 32,768 features

Purpose:
- Begin reconstruction
- Upsample spatial dimension 2Ã—
- Reduce feature channels 256â†’128
```

**Layer 2: ConvTranspose2d(128â†’64)**
```
Input:  16Ã—16Ã—128  = 32,768 features
         â†“ [ConvT 3Ã—3, stride=2, padding=1, output_padding=1]
Output: 32Ã—32Ã—64   = 65,536 features

Purpose:
- Continue upsampling
- Reconstruct mid-level features
- Reduce channels 128â†’64
```

**Layer 3: ConvTranspose2d(64â†’32)**
```
Input:  32Ã—32Ã—64   = 65,536 features
         â†“ [ConvT 3Ã—3, stride=2, padding=1, output_padding=1]
Output: 64Ã—64Ã—32   = 131,072 features

Purpose:
- Further upsampling
- Reconstruct low-level features (textures)
- Reduce channels 64â†’32
```

**Layer 4: ConvTranspose2d(32â†’3) + Sigmoid**
```
Input:  64Ã—64Ã—32   = 131,072 features
         â†“ [ConvT 3Ã—3, stride=2, padding=1, output_padding=1]
         â†“ + Sigmoid activation
Output: 128Ã—128Ã—3  = 49,152 pixels (RGB)

Purpose:
- Final reconstruction to original size
- Reduce to 3 RGB channels
- Sigmoid ensures output in [0, 1]
- Match input image format
```

**Sigmoid Activation:**
```python
output = 1 / (1 + exp(-x))
# Ensures all pixel values are in range [0, 1]
# Critical for image reconstruction
```

---

### ğŸ“ STEP 4: RECONSTRUCTION ERROR

```
Input:
â”œâ”€ X_original (128Ã—128Ã—3)     - Original normalized image
â””â”€ X_recon (128Ã—128Ã—3)        - Reconstructed image
    â†“
[Compute MSE]
MSE = mean((X_original - X_recon)Â²)
    â†“
For each pixel:
  difference = original_pixel - reconstructed_pixel
  squared_diff = differenceÂ²
    â†“
MSE = sum(all squared_diff) / total_pixels
    = sum(all squared_diff) / (128 Ã— 128 Ã— 3)
    = sum(all squared_diff) / 49,152
    â†“
Output: Single MSE value (e.g., 0.000154)
```

**Chi tiáº¿t tÃ­nh toÃ¡n MSE:**

**Step-by-step:**
```python
# 1. Calculate pixel-wise difference
diff = X_original - X_recon
# Shape: (128, 128, 3)
# Values: can be positive or negative

# 2. Square each difference
squared_diff = diff ** 2
# Shape: (128, 128, 3)
# Values: all positive (removes sign)

# 3. Sum all squared differences
total_squared_error = np.sum(squared_diff)
# Single value: sum of 49,152 squared differences

# 4. Compute mean
MSE = total_squared_error / (128 * 128 * 3)
    = total_squared_error / 49,152
```

**VÃ­ dá»¥ cá»¥ thá»ƒ:**
```
Original pixel:      [0.8, 0.6, 0.4]  (RGB)
Reconstructed pixel: [0.7, 0.5, 0.3]  (RGB)

Differences:  [0.1, 0.1, 0.1]
Squared:      [0.01, 0.01, 0.01]
Sum:          0.03

Do this for all 49,152 pixels:
Total squared error = Î£(all squared differences)
MSE = Total / 49,152
```

**Ã nghÄ©a cá»§a MSE:**
- ğŸ“‰ **MSE tháº¥p** (< 0.000312): TÃ¡i táº¡o tá»‘t â†’ CÃ³ thá»ƒ lÃ  REAL
  ```
  Original:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  Reconstructed: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â† Very similar
  MSE = 0.000154
  ```

- ğŸ“ˆ **MSE cao** (â‰¥ 0.000312): TÃ¡i táº¡o kÃ©m â†’ CÃ³ thá»ƒ lÃ  FAKE
  ```
  Original:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  Reconstructed: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆ  â† Different
  MSE = 0.000450
  ```

---

### â“ DECISION: MSE < Threshold?

```
MSE value (computed)
    â†“
Compare with Threshold = 0.000312
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IF MSE < 0.000312:  â†’ [YES]   â”‚
â”‚  ELSE:               â†’ [NO]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                    â†“
    [YES Branch]         [NO Branch]
```

**Decision Tree:**
```
                MSE < Threshold?
                      â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                         â”‚
       [YES]                     [NO]
    MSE < 0.000312          MSE â‰¥ 0.000312
         â”‚                         â”‚
    Reconstruction               Reconstruction
    is GOOD                      is POOR
         â”‚                         â”‚
      âœ… REAL                    âŒ FAKE
   (Valid Iris)               (Spoofed Iris)
```

**VÃ­ dá»¥ thá»±c táº¿:**

**Case 1: REAL iris**
```
Input: áº¢nh má»‘ng máº¯t tháº­t tá»« webcam
  â†“ Preprocessing
  â†“ AutoEncoder Forward Pass
  â†“ MSE = 0.000154
  â†“ Compare: 0.000154 < 0.000312? â†’ YES
  â†“
âœ… Result: REAL (Valid Iris)
   Confidence: High (MSE chá»‰ báº±ng 49% threshold)
```

**Case 2: FAKE iris (printed photo)**
```
Input: áº¢nh má»‘ng máº¯t in trÃªn giáº¥y
  â†“ Preprocessing
  â†“ AutoEncoder Forward Pass
  â†“ MSE = 0.000567
  â†“ Compare: 0.000567 < 0.000312? â†’ NO
  â†“
âŒ Result: FAKE (Spoofed Iris)
   Confidence: High (MSE gáº¥p 1.8Ã— threshold)
```

---

### ğŸ“¤ FINAL OUTPUT

#### Output 1: REAL (Valid Iris) âœ…
```json
{
  "status": "REAL",
  "description": "Valid Iris - Reconstruction successful",
  "mse": 0.000154,
  "threshold": 0.000312,
  "confidence": 0.95,
  "reconstruction_quality": "excellent",
  "action": "grant_access"
}
```

**Characteristics:**
- ğŸ“‰ **Low MSE**: Significantly below threshold
- âœ… **Good reconstruction**: Original â‰ˆ Reconstructed
- ğŸ”“ **Action**: Allow authentication

#### Output 2: FAKE (Spoofed Iris) âŒ
```json
{
  "status": "FAKE",
  "description": "Spoofed Iris - High reconstruction error",
  "mse": 0.000567,
  "threshold": 0.000312,
  "confidence": 0.88,
  "reconstruction_quality": "poor",
  "action": "deny_access",
  "alert": "Possible presentation attack detected"
}
```

**Characteristics:**
- ğŸ“ˆ **High MSE**: Above threshold
- âŒ **Poor reconstruction**: Original â‰  Reconstructed
- ğŸ”’ **Action**: Deny authentication, trigger alert

---

### ğŸ“Š Performance Metrics

**Latency Breakdown:**
```
Total Latency: 2.84ms (GPU) / 50ms (CPU)

â”œâ”€ Step 1 (Preprocessing):     0.5ms  (18%)
â”œâ”€ Step 2 (Normalization):     0.1ms  (4%)
â”œâ”€ Step 3 (AutoEncoder):       2.0ms  (70%)  â† Bottleneck
â””â”€ Step 4 (MSE + Decision):    0.24ms (8%)
```

**Throughput:**
```
GPU: 352 FPS (1000ms / 2.84ms)
CPU: ~25 FPS (1000ms / 50ms)
```

---

## ğŸ—ï¸ HÃŒNH 2.3: KIáº¾N TRÃšC AUTOENCODER CHI TIáº¾T

### Tá»•ng quan
HÃ¬nh nÃ y mÃ´ táº£ chi tiáº¿t **cáº¥u trÃºc bÃªn trong** cá»§a mÃ´ hÃ¬nh AutoEncoder, bao gá»“m tá»«ng layer vÃ  tham sá»‘ cá»¥ thá»ƒ.

---

### ğŸ“¥ INPUT LAYER

```
Input Shape: (Batch, 3, 128, 128)
             â†‘     â†‘   â†‘    â†‘
             â”‚     â”‚   â”‚    â””â”€ Width (pixels)
             â”‚     â”‚   â””â”€â”€â”€â”€â”€â”€ Height (pixels)
             â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Channels (RGB)
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Batch size

Example: (32, 3, 128, 128)
         = 32 images, each 3 channels, 128Ã—128 pixels
         = 32 Ã— 49,152 = 1,572,864 values total
```

**PyTorch Format:**
- ğŸ”¢ **Channel-first**: (N, C, H, W)
- ğŸ“¦ **Batch processing**: Multiple images at once
- ğŸ’¾ **Memory**: ~6 MB per batch (32 images, float32)

---

### ğŸ”½ ENCODER (Compression) - Detailed

#### Layer 1: Conv2d(3 â†’ 32)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: (Batch, 3, 128, 128)             â”‚
â”‚                                          â”‚
â”‚ Conv2d Configuration:                    â”‚
â”‚ â”œâ”€ in_channels: 3                       â”‚
â”‚ â”œâ”€ out_channels: 32                     â”‚
â”‚ â”œâ”€ kernel_size: 3Ã—3                     â”‚
â”‚ â”œâ”€ stride: 2                            â”‚
â”‚ â”œâ”€ padding: 1                           â”‚
â”‚                                          â”‚
â”‚ BatchNorm2d(32)                         â”‚
â”‚ ReLU Activation                         â”‚
â”‚                                          â”‚
â”‚ OUTPUT: (Batch, 32, 64, 64)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Parameters Calculation:**
```
Conv2d weights: (out_ch Ã— in_ch Ã— kernel_h Ã— kernel_w)
              = 32 Ã— 3 Ã— 3 Ã— 3
              = 864 weights

Conv2d bias:    32 (one per output channel)

BatchNorm:      32 Ã— 2 = 64 (gamma + beta)

Total: 864 + 32 + 64 = 960 parameters
```

**Spatial Dimension Calculation:**
```
Output_size = (Input_size + 2Ã—padding - kernel_size) / stride + 1

Height: (128 + 2Ã—1 - 3) / 2 + 1 = 127 / 2 + 1 = 64
Width:  (128 + 2Ã—1 - 3) / 2 + 1 = 127 / 2 + 1 = 64

Result: 64Ã—64 feature maps
```

**What does this layer learn?**
- ğŸ¨ **Low-level features**: Edges, corners, gradients
- ğŸ“Š **Color transitions**: RGB channel interactions
- ğŸ” **Local patterns**: Small textures (3Ã—3 receptive field)

#### Layer 2: Conv2d(32 â†’ 64)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: (Batch, 32, 64, 64)              â”‚
â”‚                                          â”‚
â”‚ Conv2d(32â†’64, kernel=3Ã—3, stride=2)     â”‚
â”‚ BatchNorm2d(64)                         â”‚
â”‚ ReLU                                     â”‚
â”‚                                          â”‚
â”‚ OUTPUT: (Batch, 64, 32, 32)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Parameters:**
```
Conv2d: 64 Ã— 32 Ã— 3 Ã— 3 = 18,432
Bias:   64
BN:     64 Ã— 2 = 128
Total:  18,624 parameters
```

**Receptive Field:**
```
Layer 1: 3Ã—3 pixels
Layer 2: 3Ã—3 on 64Ã—64 = 7Ã—7 on original 128Ã—128
```

**What does this layer learn?**
- ğŸ–¼ï¸ **Mid-level features**: Textures, small patterns
- ğŸ”„ **Feature combinations**: Combining edge features
- ğŸ“ **Iris structures**: Radial patterns, furrows

#### Layer 3: Conv2d(64 â†’ 128)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: (Batch, 64, 32, 32)              â”‚
â”‚                                          â”‚
â”‚ Conv2d(64â†’128, kernel=3Ã—3, stride=2)    â”‚
â”‚ BatchNorm2d(128)                        â”‚
â”‚ ReLU                                     â”‚
â”‚                                          â”‚
â”‚ OUTPUT: (Batch, 128, 16, 16)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Parameters:**
```
Conv2d: 128 Ã— 64 Ã— 3 Ã— 3 = 73,728
Bias:   128
BN:     128 Ã— 2 = 256
Total:  74,112 parameters
```

**Receptive Field:**
```
Layer 3: 15Ã—15 on original 128Ã—128
```

**What does this layer learn?**
- ğŸ¯ **High-level features**: Iris collarette, crypts
- ğŸ” **Complex patterns**: Multiple texture combinations
- ğŸ“Š **Iris-specific structures**: Unique identification features

#### Layer 4: Conv2d(128 â†’ 256) + Dropout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: (Batch, 128, 16, 16)             â”‚
â”‚                                          â”‚
â”‚ Conv2d(128â†’256, kernel=3Ã—3, stride=2)   â”‚
â”‚ BatchNorm2d(256)                        â”‚
â”‚ ReLU                                     â”‚
â”‚ Dropout2d(p=0.2)  â† Regularization     â”‚
â”‚                                          â”‚
â”‚ OUTPUT: (Batch, 256, 8, 8)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Parameters:**
```
Conv2d: 256 Ã— 128 Ã— 3 Ã— 3 = 294,912
Bias:   256
BN:     256 Ã— 2 = 512
Dropout: 0 (no parameters, just masking)
Total:  295,680 parameters
```

**Dropout Effect:**
```
Training:
  20% of feature maps randomly dropped
  Remaining 80% scaled by 1.25Ã— (to maintain expected output)

Inference:
  No dropout (all features active)
  Prevents overfitting during training
```

**Receptive Field:**
```
Layer 4: 31Ã—31 on original 128Ã—128
         Covers ~24% of input image
```

---

### ğŸ¯ LATENT SPACE (Bottleneck)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LATENT SPACE                   â”‚
â”‚                                          â”‚
â”‚ Dimension: 8Ã—8Ã—256 = 16,384 features   â”‚
â”‚                                          â”‚
â”‚ Dropout: 0.2 (training only)            â”‚
â”‚                                          â”‚
â”‚ Compression Ratio: 49,152 â†’ 16,384     â”‚
â”‚                    (~67% reduction)     â”‚
â”‚                                          â”‚
â”‚ Information Content:                     â”‚
â”‚ - Essential iris features only          â”‚
â”‚ - Removes redundancy                    â”‚
â”‚ - Compact representation                â”‚
â”‚ - Enables anomaly detection             â”‚
â”‚                                          â”‚
â”‚ Visualization:                          â”‚
â”‚    8Ã—8 spatial Ã— 256 channels           â”‚
â”‚    = 64 spatial positions               â”‚
â”‚    = Each position has 256 features     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why 8Ã—8Ã—256 is important:**

**1. Compression:**
```
Original: 128Ã—128Ã—3 = 49,152 values
Latent:   8Ã—8Ã—256   = 16,384 values
Ratio:    49,152 / 16,384 = 3:1
```

**2. Information Bottleneck:**
```
Forces model to learn ONLY essential features
â”œâ”€ REAL iris: Can be compressed and reconstructed well
â””â”€ FAKE iris: Cannot be compressed effectively (loses info)
```

**3. Feature Distribution:**
```
8Ã—8 grid = 64 spatial locations
Each location: 256-dimensional feature vector

Example feature map:
â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”
â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚  Each cell: 256 features
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  Represents 16Ã—16 region
â”‚ â”‚ â”‚â—â”‚â—â”‚â—â”‚â—â”‚ â”‚ â”‚  of original image
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  â— = High activation
â”‚ â”‚â—â”‚â—â”‚â—â”‚â—â”‚â—â”‚â—â”‚ â”‚    (important features)
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”‚ â”‚â—â”‚â—â”‚â—â”‚â—â”‚â—â”‚â—â”‚ â”‚  Captures:
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  - Iris patterns
â”‚ â”‚â—â”‚â—â”‚â—â”‚â—â”‚â—â”‚â—â”‚ â”‚  - Texture density
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  - Color distribution
â”‚ â”‚â—â”‚â—â”‚â—â”‚â—â”‚â—â”‚â—â”‚ â”‚  - Structural features
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”‚ â”‚ â”‚â—â”‚â—â”‚â—â”‚â—â”‚ â”‚ â”‚
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜
```

---

### ğŸ”¼ DECODER (Reconstruction) - Detailed

#### Layer 1: ConvTranspose2d(256 â†’ 128)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: (Batch, 256, 8, 8)               â”‚
â”‚                                          â”‚
â”‚ ConvTranspose2d Configuration:          â”‚
â”‚ â”œâ”€ in_channels: 256                     â”‚
â”‚ â”œâ”€ out_channels: 128                    â”‚
â”‚ â”œâ”€ kernel_size: 3Ã—3                     â”‚
â”‚ â”œâ”€ stride: 2                            â”‚
â”‚ â”œâ”€ padding: 1                           â”‚
â”‚ â””â”€ output_padding: 1                    â”‚
â”‚                                          â”‚
â”‚ BatchNorm2d(128)                        â”‚
â”‚ ReLU                                     â”‚
â”‚                                          â”‚
â”‚ OUTPUT: (Batch, 128, 16, 16)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ConvTranspose (Deconvolution) Explained:**
```
Regular Conv:     Downsample (e.g., 8Ã—8 â†’ 4Ã—4)
ConvTranspose:    Upsample (e.g., 8Ã—8 â†’ 16Ã—16)

Process:
1. Insert zeros between input pixels (stride=2)
2. Apply convolution
3. Remove padding
4. Result: 2Ã— spatial increase
```

**Parameters:**
```
ConvT: 128 Ã— 256 Ã— 3 Ã— 3 = 294,912
Bias:  128
BN:    128 Ã— 2 = 256
Total: 295,296 parameters
```

**Output size calculation:**
```
Output_size = (Input_size - 1) Ã— stride - 2Ã—padding + kernel + output_padding

Height: (8 - 1) Ã— 2 - 2Ã—1 + 3 + 1 = 16
Width:  (8 - 1) Ã— 2 - 2Ã—1 + 3 + 1 = 16
```

#### Layer 2: ConvTranspose2d(128 â†’ 64)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: (Batch, 128, 16, 16)             â”‚
â”‚                                          â”‚
â”‚ ConvTranspose2d(128â†’64, 3Ã—3, stride=2)  â”‚
â”‚ BatchNorm2d(64)                         â”‚
â”‚ ReLU                                     â”‚
â”‚                                          â”‚
â”‚ OUTPUT: (Batch, 64, 32, 32)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Parameters:**
```
ConvT: 64 Ã— 128 Ã— 3 Ã— 3 = 73,728
Bias:  64
BN:    64 Ã— 2 = 128
Total: 73,920 parameters
```

#### Layer 3: ConvTranspose2d(64 â†’ 32)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: (Batch, 64, 32, 32)              â”‚
â”‚                                          â”‚
â”‚ ConvTranspose2d(64â†’32, 3Ã—3, stride=2)   â”‚
â”‚ BatchNorm2d(32)                         â”‚
â”‚ ReLU                                     â”‚
â”‚                                          â”‚
â”‚ OUTPUT: (Batch, 32, 64, 64)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Parameters:**
```
ConvT: 32 Ã— 64 Ã— 3 Ã— 3 = 18,432
Bias:  32
BN:    32 Ã— 2 = 64
Total: 18,528 parameters
```

#### Layer 4: ConvTranspose2d(32 â†’ 3) + Sigmoid

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: (Batch, 32, 64, 64)              â”‚
â”‚                                          â”‚
â”‚ ConvTranspose2d(32â†’3, 3Ã—3, stride=2)    â”‚
â”‚ Sigmoid Activation  â† IMPORTANT         â”‚
â”‚                                          â”‚
â”‚ OUTPUT: (Batch, 3, 128, 128)           â”‚
â”‚         Values in range [0, 1]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Parameters:**
```
ConvT: 3 Ã— 32 Ã— 3 Ã— 3 = 864
Bias:  3
Total: 867 parameters
```

**Sigmoid Activation:**
```python
sigmoid(x) = 1 / (1 + exp(-x))

Properties:
- Input: any real number (-âˆ to +âˆ)
- Output: [0, 1]
- Smooth, differentiable
- Perfect for image pixels (normalized)

Example:
  x = 2.0  â†’ sigmoid(2.0)  = 0.88
  x = 0.0  â†’ sigmoid(0.0)  = 0.50
  x = -2.0 â†’ sigmoid(-2.0) = 0.12
```

---

### ğŸ“Š MODEL SUMMARY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         MODEL SUMMARY                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total Parameters: ~2.5M                  â”‚
â”‚ â”œâ”€ Encoder: ~463K                       â”‚
â”‚ â”œâ”€ Decoder: ~388K                       â”‚
â”‚ â””â”€ Total Trainable: ~777K (~0.78M)     â”‚
â”‚                                          â”‚
â”‚ Input Shape: (Batch, 3, 128, 128)       â”‚
â”‚ Output Shape: (Batch, 3, 128, 128)      â”‚
â”‚                                          â”‚
â”‚ Output Range: [0, 1] via Sigmoid        â”‚
â”‚                                          â”‚
â”‚ Memory Footprint:                        â”‚
â”‚ â”œâ”€ Model weights: ~10 MB (float32)     â”‚
â”‚ â”œâ”€ Activation maps: ~50 MB (batch=32)  â”‚
â”‚ â””â”€ Total GPU memory: ~100 MB           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Detailed Parameter Count:**
```
ENCODER:
â”œâ”€ Conv2d(3â†’32):     960
â”œâ”€ Conv2d(32â†’64):    18,624
â”œâ”€ Conv2d(64â†’128):   74,112
â””â”€ Conv2d(128â†’256):  295,680
   Subtotal:         389,376

DECODER:
â”œâ”€ ConvT2d(256â†’128): 295,296
â”œâ”€ ConvT2d(128â†’64):  73,920
â”œâ”€ ConvT2d(64â†’32):   18,528
â””â”€ ConvT2d(32â†’3):    867
   Subtotal:         388,611

TOTAL: 389,376 + 388,611 = 777,987 â‰ˆ 0.78M
```

---

### ğŸ¨ ConvTranspose2d Visualization

**How upsampling works:**

```
Input: 2Ã—2        Stride=2        Output: 4Ã—4
â”Œâ”€â”¬â”€â”            Insert zeros      â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”
â”‚1â”‚2â”‚    â”€â”€â”€>    between pixels    â”‚aâ”‚bâ”‚câ”‚dâ”‚
â”œâ”€â”¼â”€â”¤                          â”€â”€â”€>â”‚eâ”‚fâ”‚gâ”‚hâ”‚
â”‚3â”‚4â”‚            Apply 3Ã—3 kernel  â”‚iâ”‚jâ”‚kâ”‚lâ”‚
â””â”€â”´â”€â”˜                               â”‚mâ”‚nâ”‚oâ”‚pâ”‚
                                    â””â”€â”´â”€â”´â”€â”´â”€â”˜

Process:
[1, 2]      [1, 0, 2, 0]       [a, b, c, d]
[3, 4]  â†’   [0, 0, 0, 0]   â†’   [e, f, g, h]
            [3, 0, 4, 0]       [i, j, k, l]
            [0, 0, 0, 0]       [m, n, o, p]
```

---

### âš–ï¸ BatchNorm2d Explained

**What is Batch Normalization?**

```
For each channel (feature map):
1. Calculate mean (Î¼) and std (Ïƒ) across batch
2. Normalize: x_norm = (x - Î¼) / (Ïƒ + Îµ)
3. Scale and shift: y = Î³ Ã— x_norm + Î²
   (Î³ and Î² are learnable parameters)
```

**Benefits:**
- âš¡ **Faster training**: Normalizes activations
- ğŸ“Š **Stable gradients**: Prevents vanishing/exploding
- ğŸ¯ **Higher learning rates**: Can train faster
- ğŸ”„ **Regularization**: Slight regularization effect

**Example:**
```
Input feature map (8Ã—8):
[[0.1, 0.9, 0.3, ...],   Î¼ = 0.5, Ïƒ = 0.2
 [0.7, 0.2, 0.8, ...],   
 ...]

After BN:
[[âˆ’2.0, 2.0, âˆ’1.0, ...],  â† Normalized
 [1.0, âˆ’1.5, 1.5, ...],   â† Mean=0, Std=1
 ...]

After scale/shift (Î³=0.5, Î²=0.1):
[[âˆ’0.9, 1.1, âˆ’0.4, ...],  â† Î³Ã—norm + Î²
 [0.6, âˆ’0.65, 0.85, ...],
 ...]
```

---

### ğŸ¯ Dropout2d Explained

**What is Dropout?**

```
Training:
  Randomly drop 20% of feature maps
  Remaining 80% scaled by 1.25Ã—

Input: (Batch, 256, 8, 8)
       256 feature maps
  â†“
Dropout(p=0.2):
  Keep:  80% Ã— 256 = ~205 feature maps
  Drop:  20% Ã— 256 = ~51 feature maps
  Scale: Ã— 1.25 (to maintain expected output)
  â†“
Output: (Batch, 256, 8, 8)
        Same shape, but 20% channels zeroed

Inference (testing):
  No dropout (all features active)
```

**Why Dropout?**
- ğŸ›¡ï¸ **Prevents overfitting**: Model can't rely on specific features
- ğŸ”„ **Ensemble effect**: Like training multiple models
- ğŸ’ª **Robust features**: Forces learning of diverse features

**Visualization:**
```
Without Dropout:
Feature Maps: [âœ“][âœ“][âœ“][âœ“][âœ“][âœ“][âœ“][âœ“]
              All features always active
              â†’ May overfit

With Dropout (training):
Feature Maps: [âœ“][âœ—][âœ“][âœ“][âœ—][âœ“][âœ—][âœ“]
              Random 20% dropped
              â†’ Forces redundancy
              â†’ Better generalization
```

---

### ğŸ”‘ KEY INSIGHTS

#### 1. Symmetry between Encoder and Decoder
```
Encoder:     3 â†’ 32 â†’ 64 â†’ 128 â†’ 256
Decoder:   256 â†’ 128 â†’ 64 â†’ 32 â†’ 3
             â†‘                    â†‘
          Mirror structure
```

#### 2. Progressive Compression/Expansion
```
Spatial Dimensions:
128Ã—128 â†’ 64Ã—64 â†’ 32Ã—32 â†’ 16Ã—16 â†’ 8Ã—8  (Encoder)
  8Ã—8 â†’ 16Ã—16 â†’ 32Ã—32 â†’ 64Ã—64 â†’ 128Ã—128 (Decoder)

Feature Channels:
3 â†’ 32 â†’ 64 â†’ 128 â†’ 256  (Encoder: Increase features)
256 â†’ 128 â†’ 64 â†’ 32 â†’ 3  (Decoder: Decrease features)
```

#### 3. Information Flow
```
Input Image (49,152 pixels)
      â†“ Encoder compresses
Latent Space (16,384 features)  â† Bottleneck
      â†“ Decoder expands
Output Image (49,152 pixels)

Information loss happens at bottleneck:
- REAL iris: Minimal loss (essential features retained)
- FAKE iris: Significant loss (can't compress unfamiliar patterns)
```

#### 4. Why This Works for Anomaly Detection
```
REAL Iris:
  Input â†’ [Compress well] â†’ Latent â†’ [Reconstruct well] â†’ Output
  MSE between Input and Output: LOW

FAKE Iris:
  Input â†’ [Compress poorly] â†’ Latent â†’ [Reconstruct poorly] â†’ Output
  MSE between Input and Output: HIGH

Threshold separates these two cases!
```

---

## ğŸ“Š Tá»”NG Káº¾T 3 HÃŒNH

### So sÃ¡nh 3 Perspectives

| KhÃ­a cáº¡nh | HÃ¬nh 2.1 | HÃ¬nh 2.2 | HÃ¬nh 2.3 |
|-----------|----------|----------|----------|
| **GÃ³c nhÃ¬n** | System-level (toÃ n há»‡ thá»‘ng) | Process-level (quy trÃ¬nh) | Architecture-level (kiáº¿n trÃºc) |
| **Chi tiáº¿t** | High-level overview | Step-by-step flow | Layer-by-layer structure |
| **Má»¥c Ä‘Ã­ch** | Hiá»ƒu tá»•ng quan workflow | Hiá»ƒu quy trÃ¬nh xá»­ lÃ½ | Hiá»ƒu cáº¥u trÃºc mÃ´ hÃ¬nh |
| **Äá»™c giáº£** | Project managers, stakeholders | Developers, researchers | ML engineers, researchers |

### Information Flow across 3 Diagrams

```
HÃŒNH 2.1 (System View):
Training Phase â†’ Trained Model â†’ Inference Phase
                      â†“
HÃŒNH 2.2 (Process View):
Input â†’ Preprocess â†’ Normalize â†’ AutoEncoder â†’ MSE â†’ Decision
                                       â†“
HÃŒNH 2.3 (Architecture View):
Encoder (4 layers) â†’ Latent Space â†’ Decoder (4 layers)
```

### Key Concepts Unified

1. **Preprocessing Consistency**
   - HÃ¬nh 2.1: Mentioned in both phases
   - HÃ¬nh 2.2: Detailed in Step 1
   - HÃ¬nh 2.3: Defines input requirements

2. **AutoEncoder Core**
   - HÃ¬nh 2.1: Black box "AutoEncoder Model"
   - HÃ¬nh 2.2: Shows forward pass
   - HÃ¬nh 2.3: Reveals internal structure

3. **Decision Mechanism**
   - HÃ¬nh 2.1: "Calculate MSE & Compare Threshold"
   - HÃ¬nh 2.2: "MSE < Threshold?" decision point
   - HÃ¬nh 2.3: Outputs reconstruction for comparison

---

## ğŸ§­ HÃŒNH 2.4: FLOWCHART THUáº¬T TOÃN PHÃT HIá»†N LIVENESS

### Ã nghÄ©a tá»•ng quan
HÃ¬nh 2.4 lÃ  báº£n â€œtÃ³m táº¯t thuáº­t toÃ¡nâ€ cá»§a há»‡ thá»‘ng á»Ÿ cháº¿ Ä‘á»™ cháº¡y tháº­t: tá»« lÃºc **náº¡p mÃ´ hÃ¬nh**, **láº¥y áº£nh**, **tÃ¡ch vÃ¹ng máº¯t**, **tiá»n xá»­ lÃ½**, **tÃ¡i táº¡o báº±ng AutoEncoder**, tÃ­nh **lá»—i tÃ¡i táº¡o (MSE)**, sau Ä‘Ã³ so sÃ¡nh vá»›i **ngÆ°á»¡ng** Ä‘á»ƒ káº¿t luáº­n **REAL/FAKE**.

### Diá»…n giáº£i tá»«ng khá»‘i trong flowchart

1) **Load Trained Model (`autoencoder_processed_clean.pt`)**
- Náº¡p trá»ng sá»‘ Ä‘Ã£ huáº¥n luyá»‡n (encoder + decoder) vÃ  cÃ¡c tham sá»‘ cáº§n thiáº¿t.
- ÄÃ¢y lÃ  bÆ°á»›c â€œkhá»Ÿi táº¡o há»‡ thá»‘ngâ€; sau khi load xong, má»—i frame chá»‰ cáº§n inference.

2) **Capture Iris Image (Webcam or Upload)**
- Nguá»“n áº£nh cÃ³ thá»ƒ lÃ  luá»“ng webcam (real-time) hoáº·c áº£nh táº£i lÃªn (demo).
- Cháº¥t lÆ°á»£ng vÃ  â€œdomainâ€ cá»§a áº£nh nguá»“n áº£nh hÆ°á»Ÿng máº¡nh Ä‘áº¿n phÃ¢n bá»‘ MSE.

3) **Detect Eye Region (MediaPipe FaceMesh)**
- MediaPipe phÃ¡t hiá»‡n landmark khuÃ´n máº·t vÃ  suy ra vÃ¹ng máº¯t.
- **NÃºt ráº½ nhÃ¡nh â€œEye detected?â€** thá»ƒ hiá»‡n tÃ­nh thá»±c táº¿ cá»§a há»‡ thá»‘ng: cÃ³ thá»ƒ cÃ³ frame khÃ´ng báº¯t Ä‘Æ°á»£c máº¯t do quay máº·t, nhÃ¡y máº¯t, thiáº¿u sÃ¡ng.

4) **NhÃ¡nh NO: Error â†’ Retry**
- Náº¿u khÃ´ng phÃ¡t hiá»‡n máº¯t: há»‡ thá»‘ng bÃ¡o lá»—i â€œNo eye detectedâ€.
- Sau Ä‘Ã³ **Retry**: láº¥y láº¡i áº£nh/frame vÃ  cháº¡y láº¡i pipeline.

5) **NhÃ¡nh YES: Preprocessing (giá»‘ng training)**
- Bao gá»“m: crop eyebrows, apply mask, resize 128Ã—128, normalize vá» [0, 1].
- Äiá»ƒm quan trá»ng nháº¥t cá»§a AutoEncoder-based PAD lÃ : **pipeline tiá»n xá»­ lÃ½ khi cháº¡y tháº­t pháº£i giá»‘ng pipeline lÃºc huáº¥n luyá»‡n**. Náº¿u khÃ¡c (vÃ­ dá»¥ normalize khÃ¡c, mask khÃ¡c), MSE sáº½ lá»‡ch vÃ  ngÆ°á»¡ng máº¥t hiá»‡u lá»±c.

6) **AutoEncoder Forward Pass**
- ÄÆ°a áº£nh Ä‘Ã£ chuáº©n hoÃ¡ vÃ o AutoEncoder Ä‘á»ƒ tÃ¡i táº¡o: $X_{recon} = AE(X)$.

7) **Calculate Reconstruction Error (MSE)**
- TÃ­nh: $\text{MSE} = \text{mean}((X - X_{recon})^2)$.
- ÄÃ¢y lÃ  â€œÄ‘iá»ƒm báº¥t thÆ°á»ngâ€ (anomaly score):
  - REAL thÆ°á»ng cÃ³ MSE tháº¥p (tÃ¡i táº¡o tá»‘t)
  - FAKE thÆ°á»ng cÃ³ MSE cao (tÃ¡i táº¡o kÃ©m)

8) **Decision: MSE < Threshold?**
- Náº¿u **YES** â†’ **Result = REAL (Valid Iris)**
- Náº¿u **NO** â†’ **Result = FAKE (Spoofed Iris)**

### Mapping flowchart â†” triá»ƒn khai thá»±c táº¿
- Trong dá»± Ã¡n, luá»“ng nÃ y tÆ°Æ¡ng á»©ng vá»›i real-time implementation (vÃ­ dá»¥: `main_realtime_new.py`), nÆ¡i má»—i frame cháº¡y: detect â†’ preprocess â†’ inference â†’ compute MSE â†’ compare threshold â†’ hiá»ƒn thá»‹ káº¿t quáº£.

---

## ğŸ“‰ HÃŒNH 3.1: BIá»‚U Äá»’ LOSS CURVE Cá»¦A MÃ” HÃŒNH AUTOENCODER THEO Sá» EPOCH

### Biá»ƒu Ä‘á»“ thá»ƒ hiá»‡n gÃ¬?
- Trá»¥c $x$: sá»‘ epoch (sá»‘ láº§n mÃ´ hÃ¬nh â€œÄ‘i quaâ€ toÃ n bá»™ táº­p train).
- Trá»¥c $y$: loss (á»Ÿ Ä‘Ã¢y lÃ  **MSE**) â€” cÃ ng nhá» cÃ ng tá»‘t.
- ThÆ°á»ng cÃ³ 2 Ä‘Æ°á»ng:
  - **Training loss**: lá»—i tÃ¡i táº¡o trÃªn táº­p train
  - **Validation loss**: lá»—i tÃ¡i táº¡o trÃªn táº­p validation (REAL)

### CÃ¡ch Ä‘á»c loss curve Ä‘Ãºng trong AutoEncoder

1) **Giáº£m nhanh giai Ä‘oáº¡n Ä‘áº§u**
- Thá»ƒ hiá»‡n mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡c cáº¥u trÃºc cÆ¡ báº£n cá»§a áº£nh má»‘ng máº¯t (biÃªn, pattern thÃ´).

2) **Giáº£m cháº­m vÃ  dáº§n á»•n Ä‘á»‹nh vá» sau**
- Thá»ƒ hiá»‡n quÃ¡ trÃ¬nh â€œtinh chá»‰nhâ€: mÃ´ hÃ¬nh cáº£i thiá»‡n chi tiáº¿t nhá», giáº£m sai sá»‘ dáº§n.

3) **Training loss vÃ  Validation loss bÃ¡m sÃ¡t nhau**
- LÃ  dáº¥u hiá»‡u tá»‘t: mÃ´ hÃ¬nh khÃ´ng há»c váº¹t (overfitting) má»™t cÃ¡ch rÃµ rá»‡t.
- Náº¿u validation loss tÄƒng trong khi training loss váº«n giáº£m â†’ thÆ°á»ng lÃ  overfitting.

### LiÃªn há»‡ vá»›i cÃ¡c sá»‘ liá»‡u trong bÃ¡o cÃ¡o
- Training loss (initial): **0.135653**
- Training loss (final): **0.000215**
- Validation loss (best): **0.000158**
- Early stopping: **Not triggered** (tá»©c validation loss váº«n cáº£i thiá»‡n hoáº·c khÃ´ng â€œxáº¥u Ä‘iâ€ Ä‘á»§ lÃ¢u Ä‘á»ƒ dá»«ng sá»›m).

### Ã nghÄ©a thá»±c táº¿ cho PAD
Loss curve â€œÄ‘áº¹pâ€ (giáº£m Ä‘á»u, khÃ´ng vá»t tÄƒng) giÃºp Ä‘áº£m báº£o ráº±ng **MSE trÃªn áº£nh REAL** á»•n Ä‘á»‹nh vÃ  cÃ³ thá»ƒ dÃ¹ng Ä‘á»ƒ:
- Æ°á»›c lÆ°á»£ng phÃ¢n bá»‘ MSE (mean/std/percentile)
- tÃ­nh ngÆ°á»¡ng (threshold) cho anomaly detection

---

## ğŸ§© HÃŒNH 2.5: SÆ  Äá»’ TRIá»‚N KHAI (DEPLOYMENT DIAGRAM)

### Má»¥c tiÃªu cá»§a sÆ¡ Ä‘á»“
HÃ¬nh 2.5 mÃ´ táº£ há»‡ thá»‘ng á»Ÿ gÃ³c nhÃ¬n â€œtriá»ƒn khaiâ€: cÃ¡c khá»‘i cháº¡y á»Ÿ Ä‘Ã¢u, tÆ°Æ¡ng tÃ¡c dá»¯ liá»‡u nhÆ° tháº¿ nÃ o, vÃ  artifact nÃ o Ä‘Æ°á»£c dÃ¹ng láº¡i giá»¯a training vÃ  inference.

### CÃ¡c khá»‘i chÃ­nh trong sÆ¡ Ä‘á»“

1) **Training Pipeline**
- **Data Preprocessing**: chuáº©n hoÃ¡ dá»¯ liá»‡u (crop/mask/resize/normalize).
- **AutoEncoder Training**: huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i optimizer AdamW, loss MSE.
- **Model Evaluation**: Ä‘Ã¡nh giÃ¡ loss/Ä‘á»™ á»•n Ä‘á»‹nh, tÃ­nh toÃ¡n thá»‘ng kÃª MSE cho threshold.

2) **Development Environment (Google Colab + Google Drive)**
- Dataset UBIPR2, model Ä‘Ã£ train vÃ  bÃ¡o cÃ¡o (reports) Ä‘Æ°á»£c lÆ°u trÃªn **Google Drive**.
- Google Colab sá»­ dá»¥ng GPU (Tesla T4) Ä‘á»ƒ train/infer nhanh.
- â€œDriveâ€ Ä‘Ã³ng vai trÃ² kho lÆ°u trá»¯ trung tÃ¢m:
  - **Dataset UBIPR2** (nguá»“n dá»¯ liá»‡u)
  - **Trained Models** (artifact phá»¥c vá»¥ inference)
  - **Reports** (loss curve, thá»‘ng kÃª MSE, confusion matrix/ROC á»Ÿ demo)

3) **Inference System (Real-time)**
- Input tá»« **Webcam**.
- Khá»‘i **Real-time Detector** gá»“m 3 thÃ nh pháº§n chÃ­nh:
  - **MediaPipe**: detect máº·t/máº¯t
  - **OpenCV**: xá»­ lÃ½ áº£nh (crop/resize/mask)
  - **PyTorch Model**: inference AutoEncoder

4) **User Interface**
- Chá»‰ lÃ m nhiá»‡m vá»¥ hiá»ƒn thá»‹: â€œREAL/FAKEâ€, MSE, FPS/latency (tuá»³ cÃ¡ch implement).

### Luá»“ng artifact quan trá»ng
1) Train xong â†’ lÆ°u `*.pt` model lÃªn Drive.
2) Inference system (real-time) â†’ náº¡p `*.pt` model â†’ cháº¡y trÃªn webcam.
3) Threshold thÆ°á»ng Ä‘Æ°á»£c â€œauto-computedâ€ tá»« thá»‘ng kÃª MSE cá»§a áº£nh REAL; náº¿u thay Ä‘á»•i mÃ´i trÆ°á»ng (webcam khÃ¡c, Ã¡nh sÃ¡ng khÃ¡c) cÃ³ thá»ƒ cáº§n hiá»‡u chá»‰nh (calibration).

---

## ğŸ§ª HÃŒNH 3.3: ÄÃNH GIÃ PHÃ‚N LOáº I (CONFUSION MATRIX, ROC, HISTOGRAM MSE, METRICS)

### 4 thÃ nh pháº§n trong hÃ¬nh vÃ  cÃ¡ch Ä‘á»c

#### (1) Confusion Matrix
- Ma tráº­n cho tháº¥y há»‡ thá»‘ng **dá»± Ä‘oÃ¡n toÃ n bá»™ lÃ  FAKE**.
- Cá»¥ thá»ƒ vá»›i demo n=10 (REAL=5, FAKE=5):
  - FAKE dá»± Ä‘oÃ¡n Ä‘Ãºng: 5
  - REAL bá»‹ Ä‘oÃ¡n nháº§m thÃ nh FAKE: 5
  - KhÃ´ng cÃ³ máº«u nÃ o Ä‘Æ°á»£c Ä‘oÃ¡n REAL.

=> Há»‡ quáº£:
- **Accuracy = 50%** (Ä‘Ãºng háº¿t 5 FAKE, sai háº¿t 5 REAL)
- **Precision/Recall/F1 cho lá»›p REAL** (hoáº·c theo Ä‘á»‹nh nghÄ©a positive báº¡n chá»n) bá»‹ sá»¥p vá» **0** vÃ¬ khÃ´ng cÃ³ dá»± Ä‘oÃ¡n REAL.

#### (2) ROC Curve (AUC)
- ÄÆ°á»ng ROC cho tháº¥y **AUC = 1.0**.
- Ã nghÄ©a cá»§a AUC=1.0: náº¿u báº¡n thay Ä‘á»•i ngÆ°á»¡ng má»™t cÃ¡ch phÃ¹ há»£p, **Ä‘iá»ƒm sá»‘ (MSE)** cÃ³ thá»ƒ tÃ¡ch 2 nhÃ³m ráº¥t tá»‘t trong táº­p demo.

#### (3) Histogram MSE (REAL vs FAKE) + Threshold
- Biá»ƒu Ä‘á»“ histogram thá»ƒ hiá»‡n phÃ¢n bá»‘ MSE cá»§a 2 nhÃ³m (REAL vÃ  FAKE).
- ÄÆ°á»ng threshold (0.000312) náº±m ráº¥t lá»‡ch vá» bÃªn trÃ¡i (ráº¥t nhá») so vá»›i cÃ¡c cá»™t histogram trong demo.

=> Äiá»u nÃ y giáº£i thÃ­ch vÃ¬ sao â€œdá»± Ä‘oÃ¡n táº¥t cáº£ lÃ  FAKEâ€:
- Náº¿u toÃ n bá»™ MSE cá»§a cáº£ REAL vÃ  FAKE trong áº£nh upload Ä‘á»u **lá»›n hÆ¡n** 0.000312, thÃ¬ quy táº¯c $\text{MSE} < \text{threshold}$ khÃ´ng bao giá» Ä‘Ãºng â†’ khÃ´ng thá»ƒ ra REAL.

#### (4) Metrics Summary
- **Accuracy ~ 0.5** vÃ¬ dataset cÃ¢n báº±ng (5/5) vÃ  mÃ´ hÃ¬nh Ä‘oÃ¡n háº¿t vá» má»™t phÃ­a.
- **AUC = 1.0** váº«n cÃ³ thá»ƒ xáº£y ra Ä‘á»“ng thá»i vÃ¬ AUC Ä‘o â€œkháº£ nÄƒng xáº¿p háº¡ngâ€ (ranking) khi quÃ©t ngÆ°á»¡ng, khÃ´ng phá»¥ thuá»™c vÃ o má»™t ngÆ°á»¡ng cá»‘ Ä‘á»‹nh.

### VÃ¬ sao cÃ³ nghá»‹ch lÃ½ â€œAUC ráº¥t cao nhÆ°ng metrics ráº¥t tá»‡â€?
1) **Threshold mismatch (lá»‡ch ngÆ°á»¡ng)**
- NgÆ°á»¡ng 0.000312 Ä‘Æ°á»£c tÃ­nh tá»« validation REAL (UBIPR2 NIR, Ä‘iá»u kiá»‡n chuáº©n).
- áº¢nh upload thÆ°á»ng lÃ  webcam RGB, Ä‘iá»u kiá»‡n Ä‘a dáº¡ng â†’ MSE bá»‹ â€œdá»‹châ€ lÃªn cao.

2) **Domain gap (khÃ¡c miá»n dá»¯ liá»‡u)**
- Training: NIR/controlled.
- Demo: RGB/uncontrolled.
- AutoEncoder ráº¥t nháº¡y vá»›i pipeline/thiáº¿t bá»‹, nÃªn distribution MSE thay Ä‘á»•i máº¡nh.

3) **Dataset quÃ¡ nhá» (n=10)**
- KhÃ´ng cÃ³ Ã½ nghÄ©a thá»‘ng kÃª; AUC=1.0 á»Ÿ táº­p nhá» cÃ³ thá»ƒ â€œÄ‘áº¹pâ€ nhÆ°ng khÃ´ng bá»n.

### Gá»£i Ã½ cÃ¡ch diá»…n giáº£i trong bÃ¡o cÃ¡o
- Nháº¥n máº¡nh: HÃ¬nh 3.3 lÃ  **minh hoáº¡** cÆ¡ cháº¿ quyáº¿t Ä‘á»‹nh vÃ  â€œxu hÆ°á»›ng tÃ¡ch lá»›pâ€ cá»§a MSE.
- KhÃ´ng dÃ¹ng trá»±c tiáº¿p cÃ¡c metric á»Ÿ demo Ä‘á»ƒ káº¿t luáº­n hiá»‡u nÄƒng chung.
- Náº¿u muá»‘n Ä‘Ã¡nh giÃ¡ Ä‘Ãºng, cáº§n:
  - calibration threshold theo mÃ´i trÆ°á»ng triá»ƒn khai
  - táº­p test lá»›n hÆ¡n vÃ  FAKE Ä‘a dáº¡ng (print/screen/contact lensâ€¦)

---

## ğŸ’¡ PRACTICAL INSIGHTS

### Cho Developers:
- ğŸ“ **Implementation order**: Follow HÃ¬nh 2.2 (top to bottom)
- ğŸ”§ **Debugging**: Use HÃ¬nh 2.3 to inspect layer outputs
- ğŸ¯ **Optimization**: Focus on Step 3 in HÃ¬nh 2.2 (70% latency)

### Cho Researchers:
- ğŸ“Š **Experimental design**: Modify architecture in HÃ¬nh 2.3
- ğŸ”¬ **Ablation studies**: Remove components and measure impact
- ğŸ“ˆ **Improvements**: Consider VAE, attention mechanisms

### Cho Stakeholders:
- ğŸ’° **Cost**: GPU vs CPU trade-offs (HÃ¬nh 2.1)
- â±ï¸ **Performance**: Real-time capability (HÃ¬nh 2.2)
- ğŸ”’ **Security**: Anomaly detection approach (all 3 diagrams)

---

**ğŸ“… TÃ i liá»‡u Ä‘Æ°á»£c táº¡o bá»Ÿi GitHub Copilot**  
**ğŸ”— Nguá»“n: Kiáº¿n trÃºc há»‡ thá»‘ng phÃ¡t hiá»‡n liveness má»‘ng máº¯t**
